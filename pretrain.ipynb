{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Akkermansia</th>\n",
       "      <th>Alistipes</th>\n",
       "      <th>Bacteroides</th>\n",
       "      <th>Blautia</th>\n",
       "      <th>Clostridium</th>\n",
       "      <th>Collinsella</th>\n",
       "      <th>Coprococcus</th>\n",
       "      <th>Dialister</th>\n",
       "      <th>Dorea</th>\n",
       "      <th>Eubacterium</th>\n",
       "      <th>...</th>\n",
       "      <th>Lachnospira</th>\n",
       "      <th>Odoribacter</th>\n",
       "      <th>Oscillospira</th>\n",
       "      <th>Parabacteroides</th>\n",
       "      <th>Phascolarctobacterium</th>\n",
       "      <th>Prevotella</th>\n",
       "      <th>Roseburia</th>\n",
       "      <th>Ruminococcus</th>\n",
       "      <th>Subdoligranulum</th>\n",
       "      <th>Sutterella</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>3661</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>167</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3377</td>\n",
       "      <td>99</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>61</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>475</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>104</td>\n",
       "      <td>35</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3408</td>\n",
       "      <td>19</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>54</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87</td>\n",
       "      <td>1018</td>\n",
       "      <td>2108</td>\n",
       "      <td>70</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>635</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>837</td>\n",
       "      <td>311</td>\n",
       "      <td>353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>181</td>\n",
       "      <td>1535</td>\n",
       "      <td>67</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>748</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>308</td>\n",
       "      <td>115</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1625</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>6358</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>410</td>\n",
       "      <td>53</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>2</td>\n",
       "      <td>221</td>\n",
       "      <td>1338</td>\n",
       "      <td>73</td>\n",
       "      <td>204</td>\n",
       "      <td>14</td>\n",
       "      <td>39</td>\n",
       "      <td>187</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>389</td>\n",
       "      <td>17</td>\n",
       "      <td>355</td>\n",
       "      <td>271</td>\n",
       "      <td>39</td>\n",
       "      <td>324</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>727</td>\n",
       "      <td>18</td>\n",
       "      <td>60</td>\n",
       "      <td>18</td>\n",
       "      <td>76</td>\n",
       "      <td>272</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>211</td>\n",
       "      <td>0</td>\n",
       "      <td>1601</td>\n",
       "      <td>11</td>\n",
       "      <td>84</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>809</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>48</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>140</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Akkermansia  Alistipes  Bacteroides  Blautia  Clostridium  Collinsella   \n",
       "0              0        187         3661       10            7            2  \\\n",
       "1              0          0         3377       99           22           19   \n",
       "2              0          6         3408       19           37            0   \n",
       "3             87       1018         2108       70           88            1   \n",
       "4              2        181         1535       67           68            0   \n",
       "..           ...        ...          ...      ...          ...          ...   \n",
       "314            0         19         1625       14           12            0   \n",
       "315            0        112         6358       20           20            0   \n",
       "316            2        221         1338       73          204           14   \n",
       "317            9          0          727       18           60           18   \n",
       "318            0        139          809       15           27            0   \n",
       "\n",
       "     Coprococcus  Dialister  Dorea  Eubacterium  ...  Lachnospira   \n",
       "0              0          0      2            0  ...           18  \\\n",
       "1             61         18      0          131  ...           17   \n",
       "2              0          6      2            6  ...            0   \n",
       "3             58         27      3           60  ...           26   \n",
       "4             26          0      9           15  ...           11   \n",
       "..           ...        ...    ...          ...  ...          ...   \n",
       "314            3          6      1            2  ...            7   \n",
       "315            3          0      6            1  ...            4   \n",
       "316           39        187     15            7  ...            6   \n",
       "317           76        272      3           25  ...           24   \n",
       "318           21         24      1           24  ...           33   \n",
       "\n",
       "     Odoribacter  Oscillospira  Parabacteroides  Phascolarctobacterium   \n",
       "0             20            12              167                      4  \\\n",
       "1              0             0              475                      0   \n",
       "2             12             5               80                      0   \n",
       "3             36           635              144                      0   \n",
       "4              0           183              748                    148   \n",
       "..           ...           ...              ...                    ...   \n",
       "314            0             3               19                     21   \n",
       "315            0           410               53                     21   \n",
       "316           10           389               17                    355   \n",
       "317            0           150              211                      0   \n",
       "318           30           126               48                     21   \n",
       "\n",
       "     Prevotella  Roseburia  Ruminococcus  Subdoligranulum  Sutterella  \n",
       "0             2         55            38                0          89  \n",
       "1             5        104            35              121           0  \n",
       "2             3         95            54               26           0  \n",
       "3             0        837           311              353           0  \n",
       "4             0        308           115               25           0  \n",
       "..          ...        ...           ...              ...         ...  \n",
       "314           0         44            21               32          85  \n",
       "315           0         29             9               83           0  \n",
       "316         271         39           324              233           0  \n",
       "317        1601         11            84               13           0  \n",
       "318           0         14           140               70           0  \n",
       "\n",
       "[319 rows x 22 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 检查是否有GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载OTU表格文件\n",
    "otu_table = pd.read_csv('genus_rotated_f_filtered.csv', header=0, index_col=None)\n",
    "\n",
    "# # 标准化数据\n",
    "# scaler = StandardScaler()\n",
    "# otu_table_scaled = scaler.fit_transform(otu_table)\n",
    "\n",
    "# 将标准化后的数据转换回DataFrame\n",
    "otu_table_scaled_df = otu_table\n",
    "\n",
    "# 创建genus名称到唯一索引的映射\n",
    "genus_names = otu_table.columns.tolist()\n",
    "genus_to_idx = {genus: idx for idx, genus in enumerate(genus_names)}\n",
    "\n",
    "# 创建genus名称到唯一索引的映射\n",
    "genus_names = otu_table_scaled_df.columns.tolist()\n",
    "genus_to_idx = {genus: idx for idx, genus in enumerate(genus_names)}\n",
    "\n",
    "otu_table_scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mprint\u001b[39m(vocab_size)\n\u001b[1;32m     60\u001b[0m \u001b[39m# 初始化模型并移动到GPU\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m model \u001b[39m=\u001b[39m TransformerEncoderModel(vocab_size, d_model, nhead, num_layers)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     62\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# 忽略mask token的损失计算\u001b[39;00m\n\u001b[1;32m     63\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.00001\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 自定义数据集类\n",
    "class MicrobiomeDataset(Dataset):\n",
    "    def __init__(self, data, mask_prob=0.20):\n",
    "        self.data = data\n",
    "        self.mask_prob = mask_prob\n",
    "        self.genus_to_idx = genus_to_idx\n",
    "        self.vocab_size = len(genus_to_idx)\n",
    "        self.mask_token_idx = self.vocab_size  # 使用词汇表大小作为mask token索引\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        input_sample = sample.copy()\n",
    "        target_sample = sample.copy()\n",
    "        \n",
    "        # 随机mask一些位点\n",
    "        mask = np.random.rand(len(sample)) < self.mask_prob\n",
    "        input_sample[mask] = self.mask_token_idx  # 使用mask token的索引\n",
    "        \n",
    "        return torch.tensor(input_sample, dtype=torch.long), torch.tensor(target_sample, dtype=torch.long)\n",
    "\n",
    "# class TransformerEncoderModel(nn.Module):\n",
    "#     def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
    "#         super(TransformerEncoderModel, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size + 1, d_model)  # vocab_size + 1用于包含mask token\n",
    "#         encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "#         self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         x = self.transformer_encoder(x.permute(1, 0, 2))  # 调整维度以适应Transformer输入 (seq_len, batch_size, d_model)\n",
    "#         x = x.permute(1, 0, 2)  # 调整维度回 (batch_size, seq_len, d_model)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
    "        super(TransformerEncoderModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x.permute(1, 0, 2))\n",
    "        x = x.permute(1, 0, 2)\n",
    "        if return_features:\n",
    "            return x\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "# 模型参数\n",
    "vocab_size = len(genus_names)\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "print(vocab_size)\n",
    "# 初始化模型并移动到GPU\n",
    "model = TransformerEncoderModel(vocab_size, d_model, nhead, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)  # 忽略mask token的损失计算\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# 数据加载\n",
    "dataset = MicrobiomeDataset(otu_table_scaled_df.values)\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [23,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m losses\n\u001b[1;32m     21\u001b[0m \u001b[39m# 开始训练\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m losses \u001b[39m=\u001b[39m train(model, data_loader, criterion, optimizer, device)\n\u001b[1;32m     24\u001b[0m \u001b[39m# 保存模型\u001b[39;00m\n\u001b[1;32m     25\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mpretrained_transformer_encoder_model.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m input_data, target_data \u001b[39m=\u001b[39m input_data\u001b[39m.\u001b[39mto(device), target_data\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m output \u001b[39m=\u001b[39m model(input_data)\n\u001b[1;32m     12\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, vocab_size), target_data\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     13\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 48\u001b[0m, in \u001b[0;36mTransformerEncoderModel.forward\u001b[0;34m(self, x, return_features)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, return_features\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     47\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[0;32m---> 48\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(x\u001b[39m.\u001b[39;49mpermute(\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m))\n\u001b[1;32m     49\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m return_features:\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    303\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[1;32m    305\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 306\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    308\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    309\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:573\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    571\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    572\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 573\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    574\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    576\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:581\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    580\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 581\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    582\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    583\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    584\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    585\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1189\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1176\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1177\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1187\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1190\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1191\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1192\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1193\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1194\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1195\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1196\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1197\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1198\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1199\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1201\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/functional.py:5188\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   5187\u001b[0m     \u001b[39massert\u001b[39;00m in_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 5188\u001b[0m     q, k, v \u001b[39m=\u001b[39m _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n\u001b[1;32m   5189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5190\u001b[0m     \u001b[39massert\u001b[39;00m q_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/functional.py:4765\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4762\u001b[0m \u001b[39mif\u001b[39;00m k \u001b[39mis\u001b[39;00m v:\n\u001b[1;32m   4763\u001b[0m     \u001b[39mif\u001b[39;00m q \u001b[39mis\u001b[39;00m k:\n\u001b[1;32m   4764\u001b[0m         \u001b[39m# self-attention\u001b[39;00m\n\u001b[0;32m-> 4765\u001b[0m         proj \u001b[39m=\u001b[39m linear(q, w, b)\n\u001b[1;32m   4766\u001b[0m         \u001b[39m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[1;32m   4767\u001b[0m         proj \u001b[39m=\u001b[39m proj\u001b[39m.\u001b[39munflatten(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, (\u001b[39m3\u001b[39m, E))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "def train(model, data_loader, criterion, optimizer, device, epochs=128):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for input_data, target_data in data_loader:\n",
    "            # 移动数据到GPU\n",
    "            input_data, target_data = input_data.to(device), target_data.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_data)\n",
    "            loss = criterion(output.view(-1, vocab_size), target_data.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss}')\n",
    "    return losses\n",
    "\n",
    "# 开始训练\n",
    "losses = train(model, data_loader, criterion, optimizer, device)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'pretrained_transformer_encoder_model.pth')\n",
    "\n",
    "# 绘制损失图\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(losses) + 1), losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMLP(nn.Module):\n",
    "    def __init__(self, pretrained_model, input_size, hidden_size, num_classes):\n",
    "        super(EnhancedMLP, self).__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.pretrained_model(x.long(), return_features=True)  # 获取编码后的特征\n",
    "        x = x.mean(dim=1)  # 对序列维度进行平均池化\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练的TransformerEncoderModel\n",
    "pretrained_model = TransformerEncoderModel(vocab_size, d_model, nhead, num_layers)\n",
    "pretrained_model.load_state_dict(torch.load('pretrained_transformer_encoder_model.pth'))\n",
    "pretrained_model.to(device)\n",
    "pretrained_model.eval()  # 设置为评估模式\n",
    "\n",
    "# EnhancedMLP模型参数\n",
    "input_size = vocab_size * d_model  # 根据TransformerEncoderModel的输出尺寸调整\n",
    "hidden_size = 128\n",
    "num_classes = 10  # 假设有10个类别进行分类\n",
    "\n",
    "# 初始化EnhancedMLP模型并移动到GPU\n",
    "enhanced_mlp = EnhancedMLP(pretrained_model, input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(enhanced_mlp.parameters(), lr=0.0001)\n",
    "enhanced_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "df = pd.read_csv(\"NSCLC.csv\")\n",
    "# Filter out columns that contain all zeros\n",
    "# df = df.loc[:, (NSCLC != 0).any(axis=0)]\n",
    "f1 = pd.read_csv('genus_rotated_f_filtered.csv')\n",
    "# print(f1.shape)\n",
    "# Extract genus-level data\n",
    "df['Genus'] = df['#NAME'].apply(lambda x: x.split(';g__')[1].split(';')[0] if ';g__' in x else 'Unclassified')\n",
    "\n",
    "# Select the relevant columns\n",
    "genus_df = df[['Genus'] + df.columns[1:-1].tolist()]\n",
    "\n",
    "# Filter out rows with \"_unclassified\" in the Genus column\n",
    "NSCLC = genus_df[~genus_df['Genus'].str.contains('_unclassified')]\n",
    "if 'Genus' in NSCLC.columns:\n",
    "    NSCLC = NSCLC.groupby('Genus').sum().reset_index()\n",
    "else:\n",
    "    NSCLC = NSCLC  # If there is no 'genus' column, use the original data\n",
    "NSCLC = NSCLC[NSCLC['Genus'].notna() & (NSCLC['Genus'] != '')]\n",
    "NSCLC = NSCLC.loc[:, (NSCLC != 0).any(axis=0)]\n",
    "NSCLC.set_index(NSCLC.columns[0], inplace=True)\n",
    "f2 = NSCLC.transpose()\n",
    "missing_cols = [col for col in f1.columns if col not in f2.columns]\n",
    "# Add missing columns to f2 with values set to 0 using pd.concat\n",
    "f2 = pd.concat([f2, pd.DataFrame(0, index=f2.index, columns=missing_cols)], axis=1)\n",
    "# Drop columns from f2 that are not in f1\n",
    "f2 = f2[f1.columns]\n",
    "# Merge f2 to f1, keeping only the column names\n",
    "f1 = f2\n",
    "metadata  = pd.read_csv('metadata_response.csv')\n",
    "metadata.set_index(metadata.columns[0], inplace=True)\n",
    "# num_columns = len(merged_table.columns) - 1\n",
    "merged_table = f1.join(metadata, how='inner')\n",
    "# merged_table.to_csv(\"merged_table.csv\",index=False)\n",
    "# merged_table = merged_table.drop(columns=['Best response'])\n",
    "response = merged_table['Best response']\n",
    "otu_table_merge = merged_table.drop(columns=['Best response'])\n",
    "# Drop the first column if it contains sample IDs or unnecessary data\n",
    "otu_table_merge = otu_table_merge.iloc[:, 1:]\n",
    "\n",
    "# # Normalize OTU counts by total counts per sample\n",
    "# normalized_otu_counts = otu_table_merge.div(otu_table_merge.sum(axis=1), axis=0)\n",
    "\n",
    "# # Optionally, convert to percentages\n",
    "# normalized_otu_counts *= 100\n",
    "# 标准化OTU数据\n",
    "# scaler = StandardScaler()\n",
    "# otu_table_scaled = scaler.fit_transform(otu_table_merge)\n",
    "normalized_otu_counts = otu_table_merge\n",
    "# 将标准化后的数据转换回DataFrame\n",
    "# normalized_otu_counts = pd.DataFrame(otu_table_scaled, columns=otu_table_merge.columns)\n",
    "# Print to verify\n",
    "# normalized_otu_counts.to_csv(\"normalized_otu_counts.csv\",index=False)\n",
    "# Create a dictionary to map genus names to unique indices\n",
    "genus_names = normalized_otu_counts.columns.tolist()\n",
    "genus_to_idx = {genus: idx for idx, genus in enumerate(genus_names)}\n",
    "genus_names\n",
    "genus_to_idx\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'merged_table' is your DataFrame containing the response and features\n",
    "encoder = LabelEncoder()\n",
    "merged_table['Best response'] = encoder.fit_transform(merged_table['Best response'])\n",
    "\n",
    "# Separate features and target\n",
    "# features = merged_table.drop('Best response', axis=1)\n",
    "features = normalized_otu_counts\n",
    "targets = merged_table['Best response']\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OTUDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure data is returned as tensors\n",
    "        x = torch.tensor(self.features.iloc[idx].values, dtype=torch.float32)\n",
    "        y = torch.tensor(self.targets.iloc[idx], dtype=torch.long)  # Use torch.long for classification labels\n",
    "        return x, y\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Encode the 'Best response' column\n",
    "encoder = LabelEncoder()\n",
    "merged_table['Best response'] = encoder.fit_transform(merged_table['Best response'])\n",
    "\n",
    "# Split the features and targets into training and testing sets\n",
    "features_train, features_test, targets_train, targets_test = train_test_split(\n",
    "    features, targets, test_size=0.2, random_state=42)\n",
    "# print(features_test)\n",
    "train_dataset = OTUDataset(features_train, targets_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataset = OTUDataset(features_test, targets_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the enhanced model\n",
    "def train_and_evaluate(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs=256):\n",
    "    # Check if GPU is available and move the model to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    def train_model(model, criterion, optimizer, dataloader, num_epochs):\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for inputs, labels in dataloader:\n",
    "                # Move inputs and labels to GPU\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader):.4f}')\n",
    "\n",
    "    def evaluate_model(model, dataloader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                # Move inputs and labels to GPU\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, criterion, optimizer, train_dataloader, num_epochs)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    evaluate_model(model, test_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "enhanced_model = EnhancedMLP(pretrained_model, d_model, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(enhanced_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate the enhanced model\n",
    "train_and_evaluate(enhanced_model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs=64)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
