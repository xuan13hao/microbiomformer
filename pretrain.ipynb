{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "# Load the OTU table file\n",
    "otu_table = pd.read_csv('genus_rotated_f.csv', header=0, index_col=None)\n",
    "\n",
    "otu_table_proportions = otu_table.div(otu_table.sum(axis=1), axis=0)\n",
    "# Convert proportions to percentages\n",
    "otu_table_percentages = otu_table_proportions * 100\n",
    "# standardized_otu_table.to_csv('standardized_genus_rotated_f.csv', index=False)\n",
    "\n",
    "# print(standardized_otu_table.shape)\n",
    "# Create a dictionary to map genus names to unique indices\n",
    "genus_names = otu_table_percentages.columns.tolist()\n",
    "genus_to_idx = {genus: idx for idx, genus in enumerate(genus_names)}\n",
    "\n",
    "class OTUDataset(Dataset):\n",
    "    def __init__(self, data, genus_to_idx, mask_prob=0.10):\n",
    "        self.data = data\n",
    "        self.genus_to_idx = genus_to_idx\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        counts = self.data.iloc[idx].tolist()\n",
    "        input_ids = [self.genus_to_idx[genus] for genus in self.genus_to_idx]\n",
    "        masked_ids, masked_labels = self.random_mask(counts)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(counts, dtype=torch.float),\n",
    "            'masked_labels': torch.tensor(masked_labels, dtype=torch.float)\n",
    "        }\n",
    "    \n",
    "    def random_mask(self, counts):\n",
    "        masked_labels = []\n",
    "        masked_counts = counts.copy()\n",
    "        for i in range(len(counts)):\n",
    "            if np.random.rand() < self.mask_prob:\n",
    "                masked_labels.append(counts[i])\n",
    "                masked_counts[i] = -1  # Masking this value\n",
    "            else:\n",
    "                masked_labels.append(-1)  # Not a target for prediction\n",
    "        return masked_counts, masked_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleTransformerModel(nn.Module):\n",
    "#     def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, max_seq_length):\n",
    "#         super(SimpleTransformerModel, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "#         self.pos_encoder = nn.Embedding(max_seq_length, d_model)\n",
    "#         encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "#         self.regressor = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "#     def forward(self, src, masked_labels=None, compute_loss=True):\n",
    "#         # Ensure src is of type long\n",
    "#         src = src.long()  # Convert src to long if necessary\n",
    "\n",
    "#         # Positional encoding\n",
    "#         positions = torch.arange(src.size(1), device=src.device).unsqueeze(0).repeat(src.size(0), 1)\n",
    "#         src = self.embedding(src) + self.pos_encoder(positions)\n",
    "        \n",
    "#         # Encoder\n",
    "#         output = self.transformer_encoder(src)\n",
    "#         output = output[:, 0, :]  # Use the first token representation for regression\n",
    "        \n",
    "#         if compute_loss:\n",
    "#             # Regression\n",
    "#             logits = self.regressor(output) # vector\n",
    "            \n",
    "#             # Loss calculation \n",
    "#             loss = nn.MSELoss()(logits, masked_labels)\n",
    "#             #loss = nn.MSELoss()() \n",
    "            \n",
    "#             return loss, logits\n",
    "#         else:\n",
    "#             return output\n",
    "class SimpleTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, max_seq_length, relation_matrix):\n",
    "        super(SimpleTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = nn.Embedding(max_seq_length, d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        self.regressor = nn.Linear(d_model, vocab_size)\n",
    "        self.register_buffer('relation_matrix', relation_matrix)\n",
    "\n",
    "    def forward(self, src, masked_labels=None, compute_loss=True):\n",
    "        src = src.long()  # Ensure src is of type long\n",
    "\n",
    "        # Positional encoding\n",
    "        positions = torch.arange(src.size(1), device=src.device).unsqueeze(0).repeat(src.size(0), 1)\n",
    "        src = self.embedding(src) + self.pos_encoder(positions)\n",
    "        \n",
    "        # Ensure relation_matrix is on the same device as src\n",
    "        relation_matrix = self.relation_matrix.to(src.device)\n",
    "\n",
    "        # Apply relation matrix to embeddings\n",
    "        batch_size, seq_len, d_model = src.size()\n",
    "        relation_embeddings = relation_matrix.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        src = torch.bmm(relation_embeddings, src)\n",
    "        \n",
    "        # Encoder\n",
    "        output = self.transformer_encoder(src)\n",
    "        \n",
    "        output = output[:, 0, :]  # Use the first token representation for regression\n",
    "        \n",
    "        if compute_loss:\n",
    "            # Regression\n",
    "            logits = self.regressor(output)  # vector\n",
    "\n",
    "            # Loss calculation \n",
    "            mask = (masked_labels != -1)  # Create a mask to identify the positions to consider\n",
    "            masked_logits = logits[mask]\n",
    "            masked_labels = masked_labels[mask]\n",
    "            # print(masked_labels)\n",
    "            loss = nn.MSELoss()(masked_logits, masked_labels)\n",
    "            \n",
    "            return loss, logits\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'standardized_otu_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m num_encoder_layers \u001b[39m=\u001b[39m \u001b[39m6\u001b[39m\n\u001b[1;32m     41\u001b[0m dim_feedforward \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n\u001b[0;32m---> 42\u001b[0m max_seq_length \u001b[39m=\u001b[39m standardized_otu_table\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     43\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m     44\u001b[0m model \u001b[39m=\u001b[39m SimpleTransformerModel(vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, max_seq_length, relation_matrix)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'standardized_otu_table' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def generate_relation_matrix(num_species):\n",
    "    # 初始化一个单位矩阵\n",
    "    relation_matrix = np.eye(num_species)\n",
    "    \n",
    "    # 手动设置特定species之间的关系\n",
    "    # 假设 Genus_A 和 Genus_B 关系紧密，值为0.8；Genus_A 和 Genus_C 关系较疏远，值为0.1；Genus_C 和 Genus_D 关系紧密，值为0.9\n",
    "    # 其余关系设为中等程度，例如0.2\n",
    "    relation_matrix[0, 1] = 0\n",
    "    relation_matrix[1, 0] = 0\n",
    "    \n",
    "    relation_matrix[0, 2] = 0\n",
    "    relation_matrix[2, 0] = 0\n",
    "    \n",
    "    relation_matrix[2, 3] = 0\n",
    "    relation_matrix[3, 2] = 0\n",
    "    \n",
    "    relation_matrix[0, 3] = 0\n",
    "    relation_matrix[3, 0] = 0\n",
    "    \n",
    "    relation_matrix[1, 2] = 0\n",
    "    relation_matrix[2, 1] = 0\n",
    "    \n",
    "    relation_matrix[1, 3] = 0\n",
    "    relation_matrix[3, 1] = 0\n",
    "    \n",
    "    # 对矩阵进行归一化处理，使其每行和为1\n",
    "    relation_matrix = relation_matrix / relation_matrix.sum(axis=1, keepdims=True)\n",
    "    print(relation_matrix)\n",
    "    \n",
    "    return torch.tensor(relation_matrix, dtype=torch.float32)\n",
    "# Create a dataset and dataloader\n",
    "dataset = OTUDataset(otu_table_percentages, genus_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "vocab_size = len(genus_to_idx)\n",
    "relation_matrix = generate_relation_matrix(vocab_size)\n",
    "d_model = 128\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "dim_feedforward = 128\n",
    "max_seq_length = otu_table_percentages.shape[1]\n",
    "num_epochs = 64\n",
    "model = SimpleTransformerModel(vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, max_seq_length, relation_matrix).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m loss_values \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      7\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      8\u001b[0m     total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        masked_labels = batch['masked_labels'].to(device)\n",
    "\n",
    "        \n",
    "        loss, logits = model(labels, masked_labels, compute_loss=True)\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    loss_values.append(avg_loss)  \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "# \n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        masked_labels = batch['masked_labels'].to(device)\n",
    "        \n",
    "        logits = model(input_ids, compute_loss=False)\n",
    "        print(\"Predictions:\", logits)\n",
    "        \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), loss_values, marker='o', linestyle='-', color='b')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
