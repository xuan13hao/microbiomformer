{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(319, 393)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the OTU table file\n",
    "otu_table = pd.read_csv('genus_rotated_f.csv', header=0, index_col=None)\n",
    "# Drop columns if all their values are 0\n",
    "# otu_table = otu_table.loc[:, (otu_table != 0).any(axis=0)]\n",
    "# Normalize the OTU counts (skip the first column if it's the sample IDs)\n",
    "otu_table_proportions = otu_table.div(otu_table.sum(axis=1), axis=0)\n",
    "# Convert proportions to percentages\n",
    "otu_table_percentages = otu_table_proportions * 100\n",
    "print(otu_table_percentages.shape)\n",
    "# Create a dictionary to map genus names to unique indices\n",
    "genus_names = otu_table_proportions.columns.tolist()\n",
    "genus_to_idx = {genus: idx for idx, genus in enumerate(genus_names)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OTUDataset(Dataset):\n",
    "    def __init__(self, otu_table_percentages):\n",
    "        self.otu_table_percentages = otu_table_percentages.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.otu_table_percentages)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.otu_table_percentages[idx]\n",
    "        return torch.tensor(sample, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicroTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
    "        super(MicroTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/128], Loss: 9.5154\n",
      "Epoch [2/128], Loss: 8.6709\n",
      "Epoch [3/128], Loss: 8.2212\n",
      "Epoch [4/128], Loss: 7.8054\n",
      "Epoch [5/128], Loss: 7.4045\n",
      "Epoch [6/128], Loss: 7.0158\n",
      "Epoch [7/128], Loss: 6.6268\n",
      "Epoch [8/128], Loss: 6.2395\n",
      "Epoch [9/128], Loss: 5.8509\n",
      "Epoch [10/128], Loss: 5.4657\n",
      "Epoch [11/128], Loss: 5.0894\n",
      "Epoch [12/128], Loss: 4.7127\n",
      "Epoch [13/128], Loss: 4.3545\n",
      "Epoch [14/128], Loss: 4.0077\n",
      "Epoch [15/128], Loss: 3.6651\n",
      "Epoch [16/128], Loss: 3.3481\n",
      "Epoch [17/128], Loss: 3.0426\n",
      "Epoch [18/128], Loss: 2.7581\n",
      "Epoch [19/128], Loss: 2.4932\n",
      "Epoch [20/128], Loss: 2.2563\n",
      "Epoch [21/128], Loss: 2.0270\n",
      "Epoch [22/128], Loss: 1.8222\n",
      "Epoch [23/128], Loss: 1.6347\n",
      "Epoch [24/128], Loss: 1.4675\n",
      "Epoch [25/128], Loss: 1.3092\n",
      "Epoch [26/128], Loss: 1.1748\n",
      "Epoch [27/128], Loss: 1.0508\n",
      "Epoch [28/128], Loss: 0.9398\n",
      "Epoch [29/128], Loss: 0.8447\n",
      "Epoch [30/128], Loss: 0.7672\n",
      "Epoch [31/128], Loss: 0.6872\n",
      "Epoch [32/128], Loss: 0.6231\n",
      "Epoch [33/128], Loss: 0.5603\n",
      "Epoch [34/128], Loss: 0.5086\n",
      "Epoch [35/128], Loss: 0.4634\n",
      "Epoch [36/128], Loss: 0.4223\n",
      "Epoch [37/128], Loss: 0.3886\n",
      "Epoch [38/128], Loss: 0.3603\n",
      "Epoch [39/128], Loss: 0.3356\n",
      "Epoch [40/128], Loss: 0.3081\n",
      "Epoch [41/128], Loss: 0.2874\n",
      "Epoch [42/128], Loss: 0.2605\n",
      "Epoch [43/128], Loss: 0.2402\n",
      "Epoch [44/128], Loss: 0.2270\n",
      "Epoch [45/128], Loss: 0.2078\n",
      "Epoch [46/128], Loss: 0.1928\n",
      "Epoch [47/128], Loss: 0.1775\n",
      "Epoch [48/128], Loss: 0.1714\n",
      "Epoch [49/128], Loss: 0.1584\n",
      "Epoch [50/128], Loss: 0.1497\n",
      "Epoch [51/128], Loss: 0.1418\n",
      "Epoch [52/128], Loss: 0.1326\n",
      "Epoch [53/128], Loss: 0.1247\n",
      "Epoch [54/128], Loss: 0.1181\n",
      "Epoch [55/128], Loss: 0.1133\n",
      "Epoch [56/128], Loss: 0.1090\n",
      "Epoch [57/128], Loss: 0.1068\n",
      "Epoch [58/128], Loss: 0.0997\n",
      "Epoch [59/128], Loss: 0.0927\n",
      "Epoch [60/128], Loss: 0.0888\n",
      "Epoch [61/128], Loss: 0.0835\n",
      "Epoch [62/128], Loss: 0.0822\n",
      "Epoch [63/128], Loss: 0.0776\n",
      "Epoch [64/128], Loss: 0.0736\n",
      "Epoch [65/128], Loss: 0.0705\n",
      "Epoch [66/128], Loss: 0.0694\n",
      "Epoch [67/128], Loss: 0.0644\n",
      "Epoch [68/128], Loss: 0.0635\n",
      "Epoch [69/128], Loss: 0.0630\n",
      "Epoch [70/128], Loss: 0.0587\n",
      "Epoch [71/128], Loss: 0.0568\n",
      "Epoch [72/128], Loss: 0.0527\n",
      "Epoch [73/128], Loss: 0.0510\n",
      "Epoch [74/128], Loss: 0.0496\n",
      "Epoch [75/128], Loss: 0.0476\n",
      "Epoch [76/128], Loss: 0.0453\n",
      "Epoch [77/128], Loss: 0.0448\n",
      "Epoch [78/128], Loss: 0.0433\n",
      "Epoch [79/128], Loss: 0.0409\n",
      "Epoch [80/128], Loss: 0.0394\n",
      "Epoch [81/128], Loss: 0.0383\n",
      "Epoch [82/128], Loss: 0.0372\n",
      "Epoch [83/128], Loss: 0.0357\n",
      "Epoch [84/128], Loss: 0.0350\n",
      "Epoch [85/128], Loss: 0.0333\n",
      "Epoch [86/128], Loss: 0.0334\n",
      "Epoch [87/128], Loss: 0.0330\n",
      "Epoch [88/128], Loss: 0.0318\n",
      "Epoch [89/128], Loss: 0.0308\n",
      "Epoch [90/128], Loss: 0.0295\n",
      "Epoch [91/128], Loss: 0.0296\n",
      "Epoch [92/128], Loss: 0.0277\n",
      "Epoch [93/128], Loss: 0.0263\n",
      "Epoch [94/128], Loss: 0.0271\n",
      "Epoch [95/128], Loss: 0.0252\n",
      "Epoch [96/128], Loss: 0.0246\n",
      "Epoch [97/128], Loss: 0.0236\n",
      "Epoch [98/128], Loss: 0.0230\n",
      "Epoch [99/128], Loss: 0.0230\n",
      "Epoch [100/128], Loss: 0.0233\n",
      "Epoch [101/128], Loss: 0.0219\n",
      "Epoch [102/128], Loss: 0.0216\n",
      "Epoch [103/128], Loss: 0.0205\n",
      "Epoch [104/128], Loss: 0.0200\n",
      "Epoch [105/128], Loss: 0.0201\n",
      "Epoch [106/128], Loss: 0.0187\n",
      "Epoch [107/128], Loss: 0.0185\n",
      "Epoch [108/128], Loss: 0.0180\n",
      "Epoch [109/128], Loss: 0.0183\n",
      "Epoch [110/128], Loss: 0.0176\n",
      "Epoch [111/128], Loss: 0.0168\n",
      "Epoch [112/128], Loss: 0.0163\n",
      "Epoch [113/128], Loss: 0.0158\n",
      "Epoch [114/128], Loss: 0.0159\n",
      "Epoch [115/128], Loss: 0.0164\n",
      "Epoch [116/128], Loss: 0.0153\n",
      "Epoch [117/128], Loss: 0.0166\n",
      "Epoch [118/128], Loss: 0.0160\n",
      "Epoch [119/128], Loss: 0.0156\n",
      "Epoch [120/128], Loss: 0.0148\n",
      "Epoch [121/128], Loss: 0.0149\n",
      "Epoch [122/128], Loss: 0.0148\n",
      "Epoch [123/128], Loss: 0.0186\n",
      "Epoch [124/128], Loss: 0.0158\n",
      "Epoch [125/128], Loss: 0.0150\n",
      "Epoch [126/128], Loss: 0.0147\n",
      "Epoch [127/128], Loss: 0.0135\n",
      "Epoch [128/128], Loss: 0.0133\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = otu_table_percentages.shape[1]\n",
    "# print(input_dim)\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "num_layers = 3\n",
    "dropout = 0.1\n",
    "batch_size = 32\n",
    "num_epochs = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = OTUDataset(otu_table_percentages)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = MicroTransformerModel(input_dim, embed_dim, num_heads, ff_dim, num_layers, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'transformer_pretrained_otu.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MicroTransformerModel(\n",
      "  (embedding): Linear(in_features=393, out_features=128, bias=True)\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=393, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained transformer model\n",
    "pretrained_model = MicroTransformerModel(input_dim, embed_dim, num_heads, ff_dim, num_layers, dropout)\n",
    "print(pretrained_model)\n",
    "pretrained_model.load_state_dict(torch.load('transformer_pretrained_otu.pth'))\n",
    "# pretrained_model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERR2213660     1\n",
       "ERR2213665     1\n",
       "ERR2213666     1\n",
       "ERR2213669     0\n",
       "ERR2213672     0\n",
       "              ..\n",
       "SRR15373067    0\n",
       "SRR15373089    0\n",
       "SRR15373078    0\n",
       "SRR15373012    0\n",
       "SRR15373143    1\n",
       "Name: Best response, Length: 417, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Read the data into a DataFrame\n",
    "df = pd.read_csv(\"NSCLC.csv\")\n",
    "# Filter out columns that contain all zeros\n",
    "# df = df.loc[:, (NSCLC != 0).any(axis=0)]\n",
    "f1 = pd.read_csv('genus_rotated_f.csv')\n",
    "# print(f1.shape)\n",
    "# Extract genus-level data\n",
    "df['Genus'] = df['#NAME'].apply(lambda x: x.split(';g__')[1].split(';')[0] if ';g__' in x else 'Unclassified')\n",
    "\n",
    "# Select the relevant columns\n",
    "genus_df = df[['Genus'] + df.columns[1:-1].tolist()]\n",
    "\n",
    "# Filter out rows with \"_unclassified\" in the Genus column\n",
    "NSCLC = genus_df[~genus_df['Genus'].str.contains('_unclassified')]\n",
    "if 'Genus' in NSCLC.columns:\n",
    "    NSCLC = NSCLC.groupby('Genus').sum().reset_index()\n",
    "else:\n",
    "    NSCLC = NSCLC  # If there is no 'genus' column, use the original data\n",
    "NSCLC = NSCLC[NSCLC['Genus'].notna() & (NSCLC['Genus'] != '')]\n",
    "NSCLC = NSCLC.loc[:, (NSCLC != 0).any(axis=0)]\n",
    "NSCLC.set_index(NSCLC.columns[0], inplace=True)\n",
    "f2 = NSCLC.transpose()\n",
    "missing_cols = [col for col in f1.columns if col not in f2.columns]\n",
    "# Add missing columns to f2 with values set to 0 using pd.concat\n",
    "f2 = pd.concat([f2, pd.DataFrame(0, index=f2.index, columns=missing_cols)], axis=1)\n",
    "# Drop columns from f2 that are not in f1\n",
    "f2 = f2[f1.columns]\n",
    "# Merge f2 to f1, keeping only the column names\n",
    "f1 = f2\n",
    "metadata  = pd.read_csv('metadata_response.csv')\n",
    "metadata.set_index(metadata.columns[0], inplace=True)\n",
    "# num_columns = len(merged_table.columns) - 1\n",
    "merged_table = f1.join(metadata, how='inner')\n",
    "# merged_table.to_csv(\"merged_table.csv\",index=False)\n",
    "# merged_table = merged_table.drop(columns=['Best response'])\n",
    "response = merged_table['Best response']\n",
    "otu_table_merge = merged_table.drop(columns=['Best response'])\n",
    "# Drop the first column if it contains sample IDs or unnecessary data\n",
    "otu_table_merge = otu_table_merge.iloc[:, 1:]\n",
    "\n",
    "# Normalize OTU counts by total counts per sample\n",
    "normalized_otu_counts = otu_table_merge.div(otu_table_merge.sum(axis=1), axis=0)\n",
    "\n",
    "# Optionally, convert to percentages\n",
    "normalized_otu_counts *= 100\n",
    "\n",
    "# Print to verify\n",
    "# normalized_otu_counts.to_csv(\"normalized_otu_counts.csv\",index=False)\n",
    "# Create a dictionary to map genus names to unique indices\n",
    "genus_names = normalized_otu_counts.columns.tolist()\n",
    "genus_to_idx = {genus: idx for idx, genus in enumerate(genus_names)}\n",
    "genus_names\n",
    "genus_to_idx\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'merged_table' is your DataFrame containing the response and features\n",
    "encoder = LabelEncoder()\n",
    "merged_table['Best response'] = encoder.fit_transform(merged_table['Best response'])\n",
    "\n",
    "# Separate features and target\n",
    "# features = merged_table.drop('Best response', axis=1)\n",
    "features = normalized_otu_counts\n",
    "targets = merged_table['Best response']\n",
    "targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (pretrained_model): MicroTransformerModel(\n",
      "    (embedding): Identity()\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, output_dim, pretrained_model):\n",
    "        super(MLP, self).__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.pretrained_model.embedding = nn.Identity()  # Remove the embedding layer\n",
    "        self.pretrained_model.fc = nn.Identity()  # Remove the final fully connected layer\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():  # Freeze the pretrained transformer model\n",
    "            x = self.pretrained_model(x)\n",
    "        # print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters for the MLP\n",
    "hidden_dim = 128\n",
    "output_dim = len(np.unique(response))  # Number of unique classes\n",
    "# Instantiate the transformer model\n",
    "input_dim = 392\n",
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "num_layers = 6\n",
    "\n",
    "transformer_model = MicroTransformerModel(input_dim=input_dim, embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim, num_layers=num_layers)\n",
    "\n",
    "# Instantiate the MLP model\n",
    "hidden_dim = 256\n",
    "mlp_model = MLP(embed_dim=embed_dim, hidden_dim=hidden_dim, output_dim=output_dim, pretrained_model=transformer_model)\n",
    "\n",
    "# Print model summary to confirm dimensions\n",
    "print(mlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the dataset and dataloader for MLP training\n",
    "class OTUDatasetWithLabels(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features.iloc[idx].values, dtype=torch.float32)\n",
    "        y = torch.tensor(self.targets.iloc[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features and targets into training and testing sets\n",
    "features_train, features_test, targets_train, targets_test = train_test_split(\n",
    "    normalized_otu_counts, response, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = OTUDatasetWithLabels(features_train, targets_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = OTUDatasetWithLabels(features_test, targets_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "was expecting embedding dimension of 128, but got 392",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m data, labels \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m     outputs \u001b[39m=\u001b[39m mlp_model(data)\n\u001b[1;32m     13\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     14\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[97], line 14\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     13\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# Freeze the pretrained transformer model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretrained_model(x)\n\u001b[1;32m     15\u001b[0m     \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[91], line 11\u001b[0m, in \u001b[0;36mMicroTransformerModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     10\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[0;32m---> 11\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(x)\n\u001b[1;32m     12\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    303\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[1;32m    305\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 306\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    308\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    309\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:573\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    571\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    572\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 573\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    574\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    576\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:581\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    580\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 581\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    582\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    583\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    584\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    585\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1189\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1176\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1177\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1187\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1190\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1191\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1192\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1193\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1194\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1195\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1196\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1197\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1198\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1199\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1201\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/functional.py:5168\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5165\u001b[0m \u001b[39mif\u001b[39;00m is_causal:\n\u001b[1;32m   5166\u001b[0m     attn_mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 5168\u001b[0m \u001b[39massert\u001b[39;00m embed_dim \u001b[39m==\u001b[39m embed_dim_to_check, \\\n\u001b[1;32m   5169\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwas expecting embedding dimension of \u001b[39m\u001b[39m{\u001b[39;00membed_dim_to_check\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m{\u001b[39;00membed_dim\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   5170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(embed_dim, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m   5171\u001b[0m     \u001b[39m# embed_dim can be a tensor when JIT tracing\u001b[39;00m\n\u001b[1;32m   5172\u001b[0m     head_dim \u001b[39m=\u001b[39m embed_dim\u001b[39m.\u001b[39mdiv(num_heads, rounding_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrunc\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: was expecting embedding dimension of 128, but got 392"
     ]
    }
   ],
   "source": [
    "# Training settings for the MLP\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(mlp_model.parameters(), lr=0.001)\n",
    "num_epochs = 20\n",
    "\n",
    "# Training loop for the MLP\n",
    "for epoch in range(num_epochs):\n",
    "    mlp_model.train()\n",
    "    running_loss = 0.0\n",
    "    for data, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
