{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Akkermansia</th>\n",
       "      <th>Alistipes</th>\n",
       "      <th>Bacteroides</th>\n",
       "      <th>Blautia</th>\n",
       "      <th>Clostridium</th>\n",
       "      <th>Collinsella</th>\n",
       "      <th>Coprococcus</th>\n",
       "      <th>Dialister</th>\n",
       "      <th>Dorea</th>\n",
       "      <th>Eubacterium</th>\n",
       "      <th>...</th>\n",
       "      <th>Lachnospira</th>\n",
       "      <th>Odoribacter</th>\n",
       "      <th>Oscillospira</th>\n",
       "      <th>Parabacteroides</th>\n",
       "      <th>Phascolarctobacterium</th>\n",
       "      <th>Prevotella</th>\n",
       "      <th>Roseburia</th>\n",
       "      <th>Ruminococcus</th>\n",
       "      <th>Subdoligranulum</th>\n",
       "      <th>Sutterella</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.252042</td>\n",
       "      <td>-0.280749</td>\n",
       "      <td>0.271185</td>\n",
       "      <td>-0.353872</td>\n",
       "      <td>-0.534273</td>\n",
       "      <td>-0.144031</td>\n",
       "      <td>-0.431156</td>\n",
       "      <td>-0.282647</td>\n",
       "      <td>-0.297151</td>\n",
       "      <td>-0.520141</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.335104</td>\n",
       "      <td>-0.154088</td>\n",
       "      <td>-0.513226</td>\n",
       "      <td>-0.143274</td>\n",
       "      <td>-0.191679</td>\n",
       "      <td>-0.286694</td>\n",
       "      <td>-0.269590</td>\n",
       "      <td>-0.377958</td>\n",
       "      <td>-0.479261</td>\n",
       "      <td>0.274147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.252042</td>\n",
       "      <td>-0.689663</td>\n",
       "      <td>0.176186</td>\n",
       "      <td>0.571245</td>\n",
       "      <td>-0.410225</td>\n",
       "      <td>0.435834</td>\n",
       "      <td>0.188800</td>\n",
       "      <td>-0.203102</td>\n",
       "      <td>-0.500784</td>\n",
       "      <td>2.653024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.348625</td>\n",
       "      <td>-0.592377</td>\n",
       "      <td>-0.551495</td>\n",
       "      <td>0.406199</td>\n",
       "      <td>-0.202744</td>\n",
       "      <td>-0.282424</td>\n",
       "      <td>-0.137936</td>\n",
       "      <td>-0.388103</td>\n",
       "      <td>0.088872</td>\n",
       "      <td>-0.399150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.252042</td>\n",
       "      <td>-0.676542</td>\n",
       "      <td>0.186555</td>\n",
       "      <td>-0.260320</td>\n",
       "      <td>-0.286178</td>\n",
       "      <td>-0.212250</td>\n",
       "      <td>-0.431156</td>\n",
       "      <td>-0.256132</td>\n",
       "      <td>-0.297151</td>\n",
       "      <td>-0.374805</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.578484</td>\n",
       "      <td>-0.329404</td>\n",
       "      <td>-0.535550</td>\n",
       "      <td>-0.298482</td>\n",
       "      <td>-0.202744</td>\n",
       "      <td>-0.285271</td>\n",
       "      <td>-0.162118</td>\n",
       "      <td>-0.323855</td>\n",
       "      <td>-0.357183</td>\n",
       "      <td>-0.399150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.188129</td>\n",
       "      <td>1.536405</td>\n",
       "      <td>-0.248299</td>\n",
       "      <td>0.269803</td>\n",
       "      <td>0.135584</td>\n",
       "      <td>-0.178140</td>\n",
       "      <td>0.158310</td>\n",
       "      <td>-0.163329</td>\n",
       "      <td>-0.195335</td>\n",
       "      <td>0.933217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.226935</td>\n",
       "      <td>0.196543</td>\n",
       "      <td>1.473549</td>\n",
       "      <td>-0.184306</td>\n",
       "      <td>-0.202744</td>\n",
       "      <td>-0.289541</td>\n",
       "      <td>1.831488</td>\n",
       "      <td>0.545176</td>\n",
       "      <td>1.178186</td>\n",
       "      <td>-0.399150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.218934</td>\n",
       "      <td>-0.293869</td>\n",
       "      <td>-0.439969</td>\n",
       "      <td>0.238619</td>\n",
       "      <td>-0.029813</td>\n",
       "      <td>-0.212250</td>\n",
       "      <td>-0.166913</td>\n",
       "      <td>-0.282647</td>\n",
       "      <td>0.415565</td>\n",
       "      <td>-0.156802</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.429752</td>\n",
       "      <td>-0.592377</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.893232</td>\n",
       "      <td>0.206681</td>\n",
       "      <td>-0.289541</td>\n",
       "      <td>0.410171</td>\n",
       "      <td>-0.117587</td>\n",
       "      <td>-0.361878</td>\n",
       "      <td>-0.399150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>-0.252042</td>\n",
       "      <td>-0.648115</td>\n",
       "      <td>-0.409864</td>\n",
       "      <td>-0.312293</td>\n",
       "      <td>-0.492923</td>\n",
       "      <td>-0.212250</td>\n",
       "      <td>-0.400667</td>\n",
       "      <td>-0.256132</td>\n",
       "      <td>-0.398968</td>\n",
       "      <td>-0.471696</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.483836</td>\n",
       "      <td>-0.592377</td>\n",
       "      <td>-0.541928</td>\n",
       "      <td>-0.407307</td>\n",
       "      <td>-0.144650</td>\n",
       "      <td>-0.289541</td>\n",
       "      <td>-0.299145</td>\n",
       "      <td>-0.435443</td>\n",
       "      <td>-0.329011</td>\n",
       "      <td>0.243887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>-0.252042</td>\n",
       "      <td>-0.444752</td>\n",
       "      <td>1.173340</td>\n",
       "      <td>-0.249926</td>\n",
       "      <td>-0.426765</td>\n",
       "      <td>-0.212250</td>\n",
       "      <td>-0.400667</td>\n",
       "      <td>-0.282647</td>\n",
       "      <td>0.110115</td>\n",
       "      <td>-0.495919</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.524400</td>\n",
       "      <td>-0.592377</td>\n",
       "      <td>0.756013</td>\n",
       "      <td>-0.346650</td>\n",
       "      <td>-0.144650</td>\n",
       "      <td>-0.289541</td>\n",
       "      <td>-0.339447</td>\n",
       "      <td>-0.476020</td>\n",
       "      <td>-0.089550</td>\n",
       "      <td>-0.399150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>-0.218934</td>\n",
       "      <td>-0.206400</td>\n",
       "      <td>-0.505867</td>\n",
       "      <td>0.300986</td>\n",
       "      <td>1.094884</td>\n",
       "      <td>0.265286</td>\n",
       "      <td>-0.034791</td>\n",
       "      <td>0.543739</td>\n",
       "      <td>1.026464</td>\n",
       "      <td>-0.350583</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.497358</td>\n",
       "      <td>-0.373232</td>\n",
       "      <td>0.689044</td>\n",
       "      <td>-0.410875</td>\n",
       "      <td>0.779323</td>\n",
       "      <td>0.096167</td>\n",
       "      <td>-0.312579</td>\n",
       "      <td>0.589134</td>\n",
       "      <td>0.614748</td>\n",
       "      <td>-0.399150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>-0.103058</td>\n",
       "      <td>-0.689663</td>\n",
       "      <td>-0.710248</td>\n",
       "      <td>-0.270715</td>\n",
       "      <td>-0.095972</td>\n",
       "      <td>0.401725</td>\n",
       "      <td>0.341248</td>\n",
       "      <td>0.919368</td>\n",
       "      <td>-0.195335</td>\n",
       "      <td>0.085425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253977</td>\n",
       "      <td>-0.592377</td>\n",
       "      <td>-0.073138</td>\n",
       "      <td>-0.064778</td>\n",
       "      <td>-0.202744</td>\n",
       "      <td>1.989124</td>\n",
       "      <td>-0.387809</td>\n",
       "      <td>-0.222412</td>\n",
       "      <td>-0.418222</td>\n",
       "      <td>-0.399150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>-0.252042</td>\n",
       "      <td>-0.385710</td>\n",
       "      <td>-0.682819</td>\n",
       "      <td>-0.301899</td>\n",
       "      <td>-0.368876</td>\n",
       "      <td>-0.212250</td>\n",
       "      <td>-0.217729</td>\n",
       "      <td>-0.176587</td>\n",
       "      <td>-0.398968</td>\n",
       "      <td>0.061202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132287</td>\n",
       "      <td>0.065056</td>\n",
       "      <td>-0.149675</td>\n",
       "      <td>-0.355570</td>\n",
       "      <td>-0.144650</td>\n",
       "      <td>-0.289541</td>\n",
       "      <td>-0.379749</td>\n",
       "      <td>-0.033051</td>\n",
       "      <td>-0.150589</td>\n",
       "      <td>-0.399150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Akkermansia  Alistipes  Bacteroides   Blautia  Clostridium  Collinsella   \n",
       "0      -0.252042  -0.280749     0.271185 -0.353872    -0.534273    -0.144031  \\\n",
       "1      -0.252042  -0.689663     0.176186  0.571245    -0.410225     0.435834   \n",
       "2      -0.252042  -0.676542     0.186555 -0.260320    -0.286178    -0.212250   \n",
       "3       1.188129   1.536405    -0.248299  0.269803     0.135584    -0.178140   \n",
       "4      -0.218934  -0.293869    -0.439969  0.238619    -0.029813    -0.212250   \n",
       "..           ...        ...          ...       ...          ...          ...   \n",
       "314    -0.252042  -0.648115    -0.409864 -0.312293    -0.492923    -0.212250   \n",
       "315    -0.252042  -0.444752     1.173340 -0.249926    -0.426765    -0.212250   \n",
       "316    -0.218934  -0.206400    -0.505867  0.300986     1.094884     0.265286   \n",
       "317    -0.103058  -0.689663    -0.710248 -0.270715    -0.095972     0.401725   \n",
       "318    -0.252042  -0.385710    -0.682819 -0.301899    -0.368876    -0.212250   \n",
       "\n",
       "     Coprococcus  Dialister     Dorea  Eubacterium  ...  Lachnospira   \n",
       "0      -0.431156  -0.282647 -0.297151    -0.520141  ...    -0.335104  \\\n",
       "1       0.188800  -0.203102 -0.500784     2.653024  ...    -0.348625   \n",
       "2      -0.431156  -0.256132 -0.297151    -0.374805  ...    -0.578484   \n",
       "3       0.158310  -0.163329 -0.195335     0.933217  ...    -0.226935   \n",
       "4      -0.166913  -0.282647  0.415565    -0.156802  ...    -0.429752   \n",
       "..           ...        ...       ...          ...  ...          ...   \n",
       "314    -0.400667  -0.256132 -0.398968    -0.471696  ...    -0.483836   \n",
       "315    -0.400667  -0.282647  0.110115    -0.495919  ...    -0.524400   \n",
       "316    -0.034791   0.543739  1.026464    -0.350583  ...    -0.497358   \n",
       "317     0.341248   0.919368 -0.195335     0.085425  ...    -0.253977   \n",
       "318    -0.217729  -0.176587 -0.398968     0.061202  ...    -0.132287   \n",
       "\n",
       "     Odoribacter  Oscillospira  Parabacteroides  Phascolarctobacterium   \n",
       "0      -0.154088     -0.513226        -0.143274              -0.191679  \\\n",
       "1      -0.592377     -0.551495         0.406199              -0.202744   \n",
       "2      -0.329404     -0.535550        -0.298482              -0.202744   \n",
       "3       0.196543      1.473549        -0.184306              -0.202744   \n",
       "4      -0.592377      0.032100         0.893232               0.206681   \n",
       "..           ...           ...              ...                    ...   \n",
       "314    -0.592377     -0.541928        -0.407307              -0.144650   \n",
       "315    -0.592377      0.756013        -0.346650              -0.144650   \n",
       "316    -0.373232      0.689044        -0.410875               0.779323   \n",
       "317    -0.592377     -0.073138        -0.064778              -0.202744   \n",
       "318     0.065056     -0.149675        -0.355570              -0.144650   \n",
       "\n",
       "     Prevotella  Roseburia  Ruminococcus  Subdoligranulum  Sutterella  \n",
       "0     -0.286694  -0.269590     -0.377958        -0.479261    0.274147  \n",
       "1     -0.282424  -0.137936     -0.388103         0.088872   -0.399150  \n",
       "2     -0.285271  -0.162118     -0.323855        -0.357183   -0.399150  \n",
       "3     -0.289541   1.831488      0.545176         1.178186   -0.399150  \n",
       "4     -0.289541   0.410171     -0.117587        -0.361878   -0.399150  \n",
       "..          ...        ...           ...              ...         ...  \n",
       "314   -0.289541  -0.299145     -0.435443        -0.329011    0.243887  \n",
       "315   -0.289541  -0.339447     -0.476020        -0.089550   -0.399150  \n",
       "316    0.096167  -0.312579      0.589134         0.614748   -0.399150  \n",
       "317    1.989124  -0.387809     -0.222412        -0.418222   -0.399150  \n",
       "318   -0.289541  -0.379749     -0.033051        -0.150589   -0.399150  \n",
       "\n",
       "[319 rows x 22 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "otu_table = pd.read_csv('genus_rotated_f_filtered.csv', header=0, index_col=None)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "otu_table_scaled = scaler.fit_transform(otu_table)\n",
    "\n",
    "otu_table_scaled_df = pd.DataFrame(otu_table_scaled, columns=otu_table.columns)\n",
    "\n",
    "\n",
    "genus_names = otu_table_scaled_df.columns.tolist()\n",
    "genus_to_idx = {genus: idx for idx, genus in enumerate(genus_names)}\n",
    "\n",
    "\n",
    "otu_table_scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OTUDataset(Dataset):\n",
    "    def __init__(self, csv, mask_prob=0.15):\n",
    "        self.data = csv\n",
    "        self.mask_prob = mask_prob\n",
    "        self.data_tensor = torch.tensor(self.data.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data_tensor[idx]\n",
    "        masked_sample, mask = self.mask_data(sample)\n",
    "        return masked_sample, sample, mask\n",
    "\n",
    "    def mask_data(self, sample):\n",
    "        mask = torch.bernoulli(torch.full(sample.shape, self.mask_prob)).bool()\n",
    "        masked_sample = sample.clone()\n",
    "        masked_sample[mask] = 0  # Using 0 as a placeholder for masked values\n",
    "        return masked_sample, mask\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
    "        super(TransformerEncoderModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # include mask + 1\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        x= x.long()\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x.permute(1, 0, 2))\n",
    "        x = x.permute(1, 0, 2)\n",
    "        # print(x.shape)\n",
    "        x = x.mean(dim = 1) # 2d, batch ,sequence length, dim \n",
    "        # print(x.shape)\n",
    "        if return_features:\n",
    "            return x\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "vocab_size = len(genus_names)\n",
    "d_model = 64\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "# print(vocab_size)\n",
    "\n",
    "model = TransformerEncoderModel(vocab_size, d_model, nhead, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # 忽略mask token的损失计算\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create a dataset and dataloader\n",
    "dataset = OTUDataset(otu_table_scaled_df)\n",
    "# print(dataset)\n",
    "data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "# for  masked_data, original_data, mask in data_loader:\n",
    "#     print(\"masked_data: \",masked_data)\n",
    "#     print(\"original_data: \",original_data[mask])\n",
    "#     print(\"mask: \",mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32, Loss: nan\n",
      "Epoch 2/32, Loss: nan\n",
      "Epoch 3/32, Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mavg_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m losses\n\u001b[0;32m---> 36\u001b[0m losses \u001b[39m=\u001b[39m train(model, data_loader, criterion, optimizer, device)\n\u001b[1;32m     39\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mpretrained_transformer_encoder_model.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     42\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[73], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs[mask], original_data[mask])\n\u001b[1;32m     26\u001b[0m \u001b[39m# loss = criterion(output, target_data)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     28\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "output = torch.tensor([\n",
    "    [0.1, 0.2, 0.4, 0.1, 0.2],\n",
    "    [0.3, 0.1, 0.2, 0.3, 0.1],\n",
    "    [0.2, 0.3, 0.1, 0.2, 0.2],\n",
    "    [0.3, 0.2, 0.2, 0.1, 0.2],\n",
    "    [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "    [0.1, 0.3, 0.2, 0.3, 0.1]\n",
    "])\n",
    "loss = -[log(0.4)+log(0.3)+log(0.2)+log(0.2)+log(0.2)+log(0.3)]\n",
    "target_data = torch.tensor([2, 3, 4, 1, 2, 3])\n",
    "'''\n",
    "def train(model, data_loader, criterion, optimizer, device, epochs=32):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for  masked_data, original_data, mask in data_loader:\n",
    "            masked_data, original_data, mask = masked_data.to(device), original_data.to(device), mask.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(masked_data)\n",
    "            # print(\"outputs = \",outputs[mask])\n",
    "            # print(\"original_data= \",original_data[mask])\n",
    "            # print(\"mask = \",mask)\n",
    "            loss = criterion(outputs[mask], original_data[mask])\n",
    "            # loss = criterion(output, target_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss}')\n",
    "    return losses\n",
    "\n",
    "\n",
    "losses = train(model, data_loader, criterion, optimizer, device)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'pretrained_transformer_encoder_model.pth')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(losses) + 1), losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMLP(nn.Module):\n",
    "    def __init__(self, pretrained_model, input_size, hidden_size, num_classes):\n",
    "        super(EnhancedMLP, self).__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.pretrained_model(x.long(), return_features=True)  # 获取编码后的特征\n",
    "        x = x.mean(dim=1)  # 对序列维度进行平均池化\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedMLP(\n",
       "  (pretrained_model): TransformerEncoderModel(\n",
       "    (embedding): Embedding(23, 512)\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=512, out_features=22, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=11264, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载预训练的TransformerEncoderModel\n",
    "pretrained_model = TransformerEncoderModel(vocab_size, d_model, nhead, num_layers)\n",
    "pretrained_model.load_state_dict(torch.load('pretrained_transformer_encoder_model.pth'))\n",
    "pretrained_model.to(device)\n",
    "pretrained_model.eval()  # 设置为评估模式\n",
    "\n",
    "# EnhancedMLP模型参数\n",
    "input_size = vocab_size * d_model  # 根据TransformerEncoderModel的输出尺寸调整\n",
    "hidden_size = 128\n",
    "num_classes = 10  # 假设有10个类别进行分类\n",
    "\n",
    "# 初始化EnhancedMLP模型并移动到GPU\n",
    "enhanced_mlp = EnhancedMLP(pretrained_model, input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(enhanced_mlp.parameters(), lr=0.0001)\n",
    "enhanced_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERR2213660     1\n",
       "ERR2213665     1\n",
       "ERR2213666     1\n",
       "ERR2213669     0\n",
       "ERR2213672     0\n",
       "              ..\n",
       "SRR15373067    0\n",
       "SRR15373089    0\n",
       "SRR15373078    0\n",
       "SRR15373012    0\n",
       "SRR15373143    1\n",
       "Name: Best response, Length: 417, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "df = pd.read_csv(\"NSCLC.csv\")\n",
    "# Filter out columns that contain all zeros\n",
    "# df = df.loc[:, (NSCLC != 0).any(axis=0)]\n",
    "f1 = pd.read_csv('genus_rotated_f_filtered.csv')\n",
    "# print(f1.shape)\n",
    "# Extract genus-level data\n",
    "df['Genus'] = df['#NAME'].apply(lambda x: x.split(';g__')[1].split(';')[0] if ';g__' in x else 'Unclassified')\n",
    "\n",
    "# Select the relevant columns\n",
    "genus_df = df[['Genus'] + df.columns[1:-1].tolist()]\n",
    "\n",
    "# Filter out rows with \"_unclassified\" in the Genus column\n",
    "NSCLC = genus_df[~genus_df['Genus'].str.contains('_unclassified')]\n",
    "if 'Genus' in NSCLC.columns:\n",
    "    NSCLC = NSCLC.groupby('Genus').sum().reset_index()\n",
    "else:\n",
    "    NSCLC = NSCLC  # If there is no 'genus' column, use the original data\n",
    "NSCLC = NSCLC[NSCLC['Genus'].notna() & (NSCLC['Genus'] != '')]\n",
    "NSCLC = NSCLC.loc[:, (NSCLC != 0).any(axis=0)]\n",
    "NSCLC.set_index(NSCLC.columns[0], inplace=True)\n",
    "f2 = NSCLC.transpose()\n",
    "missing_cols = [col for col in f1.columns if col not in f2.columns]\n",
    "# Add missing columns to f2 with values set to 0 using pd.concat\n",
    "f2 = pd.concat([f2, pd.DataFrame(0, index=f2.index, columns=missing_cols)], axis=1)\n",
    "# Drop columns from f2 that are not in f1\n",
    "f2 = f2[f1.columns]\n",
    "# Merge f2 to f1, keeping only the column names\n",
    "f1 = f2\n",
    "metadata  = pd.read_csv('metadata_response.csv')\n",
    "metadata.set_index(metadata.columns[0], inplace=True)\n",
    "# num_columns = len(merged_table.columns) - 1\n",
    "merged_table = f1.join(metadata, how='inner')\n",
    "# merged_table.to_csv(\"merged_table.csv\",index=False)\n",
    "# merged_table = merged_table.drop(columns=['Best response'])\n",
    "response = merged_table['Best response']\n",
    "otu_table_merge = merged_table.drop(columns=['Best response'])\n",
    "# Drop the first column if it contains sample IDs or unnecessary data\n",
    "otu_table_merge = otu_table_merge.iloc[:, 1:]\n",
    "\n",
    "# # Normalize OTU counts by total counts per sample\n",
    "# normalized_otu_counts = otu_table_merge.div(otu_table_merge.sum(axis=1), axis=0)\n",
    "\n",
    "# # Optionally, convert to percentages\n",
    "# normalized_otu_counts *= 100\n",
    "# 标准化OTU数据\n",
    "scaler = StandardScaler()\n",
    "otu_table_scaled = scaler.fit_transform(otu_table_merge)\n",
    "# normalized_otu_counts = otu_table_merge\n",
    "# 将标准化后的数据转换回DataFrame\n",
    "normalized_otu_counts = pd.DataFrame(otu_table_scaled, columns=otu_table_merge.columns)\n",
    "# Print to verify\n",
    "# normalized_otu_counts.to_csv(\"normalized_otu_counts.csv\",index=False)\n",
    "# Create a dictionary to map genus names to unique indices\n",
    "genus_names = normalized_otu_counts.columns.tolist()\n",
    "genus_to_idx = {genus: idx for idx, genus in enumerate(genus_names)}\n",
    "genus_names\n",
    "genus_to_idx\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'merged_table' is your DataFrame containing the response and features\n",
    "encoder = LabelEncoder()\n",
    "merged_table['Best response'] = encoder.fit_transform(merged_table['Best response'])\n",
    "\n",
    "# Separate features and target\n",
    "# features = merged_table.drop('Best response', axis=1)\n",
    "features = normalized_otu_counts\n",
    "targets = merged_table['Best response']\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OTUDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure data is returned as tensors\n",
    "        x = torch.tensor(self.features.iloc[idx].values, dtype=torch.float32)\n",
    "        y = torch.tensor(self.targets.iloc[idx], dtype=torch.long)  # Use torch.long for classification labels\n",
    "        return x, y\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Encode the 'Best response' column\n",
    "encoder = LabelEncoder()\n",
    "merged_table['Best response'] = encoder.fit_transform(merged_table['Best response'])\n",
    "\n",
    "# Split the features and targets into training and testing sets\n",
    "features_train, features_test, targets_train, targets_test = train_test_split(\n",
    "    features, targets, test_size=0.2, random_state=42)\n",
    "# print(features_test)\n",
    "train_dataset = OTUDataset(features_train, targets_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataset = OTUDataset(features_test, targets_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the enhanced model\n",
    "def train_and_evaluate(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs=256):\n",
    "    # Check if GPU is available and move the model to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    def train_model(model, criterion, optimizer, dataloader, num_epochs):\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for inputs, labels in dataloader:\n",
    "                # Move inputs and labels to GPU\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader):.4f}')\n",
    "\n",
    "    def evaluate_model(model, dataloader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                # Move inputs and labels to GPU\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, criterion, optimizer, train_dataloader,device,num_epochs)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    evaluate_model(model, test_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[39m=\u001b[39m Adam(enhanced_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Train and evaluate the enhanced model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m train_and_evaluate(enhanced_model, train_dataloader, test_dataloader, criterion, optimizer,num_epochs\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_and_evaluate\u001b[39m(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[39m# Check if GPU is available and move the model to GPU\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      7\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(model, criterion, optimizer, dataloader, num_epochs):\n\u001b[1;32m      8\u001b[0m         model\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "enhanced_model = EnhancedMLP(pretrained_model, d_model, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(enhanced_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate the enhanced model\n",
    "train_and_evaluate(enhanced_model, train_dataloader, test_dataloader, criterion, optimizer,num_epochs=64)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
