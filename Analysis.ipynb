{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Replace 'otu_file' with the path to your TSV file\n",
    "otu_file = 'otu_table_psn_v35.txt'\n",
    "# wget http://downloads.hmpdacc.org/data/HMQCP/otu_table_psn_v35.txt.gz\n",
    "# Read the TSV file while skipping the first line\n",
    "otu_data = pd.read_csv(otu_file, sep='\\t', skiprows=1)\n",
    "\n",
    "# otu_data.to_csv(\"example.csv\", index=False)\n",
    "\n",
    "# otu_data = pd.read_csv(\"path_to_your_output_file.csv\")\n",
    "otu_data.columns.values[0] = 'otu_id'\n",
    "\n",
    "# Define a function to parse the taxonomic data\n",
    "def parse_tax_data(otu_data, class_cols, class_regex, class_key, class_sep):\n",
    "    # Extract class information based on class_cols and class_sep\n",
    "    tax_data = otu_data[class_cols].str.split(class_sep, expand=True)\n",
    "    \n",
    "    # Create a DataFrame for taxonomic classification\n",
    "    tax_data.columns = [f'{class_key[\"hmp_rank\"]}_{i}' for i in range(tax_data.shape[1])]\n",
    "    tax_data[class_key[\"hmp_tax\"]] = tax_data.apply(lambda row: class_sep.join(row.dropna()), axis=1)\n",
    "    \n",
    "    # Merge taxonomic data back with the original OTU data\n",
    "    otu_data = otu_data.drop(columns=[class_cols])\n",
    "    otu_data = pd.concat([otu_data, tax_data], axis=1)\n",
    "    \n",
    "    return otu_data\n",
    "\n",
    "# Apply the function to parse taxonomic data\n",
    "hmp_data = parse_tax_data(otu_data, \"Consensus Lineage\", r\"([a-z]{0,1})_{0,2}(.*)$\", {\"hmp_rank\": \"taxon_rank\", \"hmp_tax\": \"taxon_name\"}, \";\")\n",
    "\n",
    "# # Remove the regex match table (not needed in this case)\n",
    "otu_data = hmp_data.drop(columns=[col for col in hmp_data.columns if col.startswith('taxon_rank_')])\n",
    "\n",
    "# # Rename the abundance matrix to something more understandable\n",
    "otu_data.columns = ['otu_count' if col == 'otu_id' else col for col in otu_data.columns]\n",
    "# Select the top 1000 rows\n",
    "# top_1000_hmp_data = hmp_data.head(5)\n",
    "\n",
    "\n",
    "# Extract the suffix of 'g__' in the 'taxon_name' column\n",
    "otu_data['genus'] = otu_data['taxon_name'].str.extract(r'g__([^;]*)')\n",
    "otu_data = otu_data[otu_data['genus'].notna() & (otu_data['genus'] != '')]\n",
    "unique_genera = otu_data['genus'].unique().tolist()\n",
    "unique_genera = unique_genera.sort()\n",
    "unique_genera\n",
    "# Delete the 'taxon_name' column\n",
    "# otu_data = otu_data.drop(columns=['taxon_name'])\n",
    "\n",
    "# # Create the desired table\n",
    "# genus_data = otu_data.drop(columns=['otu_id']).set_index('genus').T\n",
    "# genus_data.index.name = 'Sample ID'\n",
    "\n",
    "# # Save the new table to a CSV file\n",
    "# genus_data.to_csv('genus_abundance_table.csv')\n",
    "\n",
    "# # Display the first few rows of the new table\n",
    "# genus_data.head()\n",
    "# # # Delete the 'taxon_name' column\n",
    "# otu_data = otu_data.drop(columns=['taxon_name'])\n",
    "\n",
    "# otu_data = otu_data.drop(columns=['otu_count'])\n",
    "# otu_data = otu_data.sort_values(by='genus')\n",
    "# otu_data = otu_data.set_index('genus')\n",
    "# otu_data = otu_data.reset_index()\n",
    "# otu_data = otu_data.transpose()\n",
    "# otu_data.columns = otu_data.iloc[0]\n",
    "# otu_data = otu_data[1:]\n",
    "# print(otu_data.shape)\n",
    "# otu_data.to_csv(\"genus_rotated.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Load the OTU table file\n",
    "otu_table = pd.read_csv('genus_rotated.csv', header=0, index_col=0)\n",
    "# print(otu_table.head(2))\n",
    "# Create a dictionary to map genus names to unique indices\n",
    "genus_names = otu_table.columns.tolist()\n",
    "genus_to_idx = {genus: idx for idx, genus in enumerate(genus_names)}\n",
    "\n",
    "# print(genus_to_idx)\n",
    "# Define a dataset class\n",
    "class OTUDataset(Dataset):\n",
    "    def __init__(self, data, genus_to_idx):\n",
    "        self.data = data\n",
    "        self.genus_to_idx = genus_to_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        counts = self.data.iloc[idx].tolist()\n",
    "        input_ids = [self.genus_to_idx[genus] for genus in self.genus_to_idx]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(counts, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Create a dataset and dataloader\n",
    "dataset = OTUDataset(otu_table, genus_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 9.5476\n",
      "Epoch 2/300, Loss: 9.5474\n",
      "Epoch 3/300, Loss: 9.5375\n",
      "Epoch 4/300, Loss: 9.5353\n",
      "Epoch 5/300, Loss: 9.5142\n",
      "Epoch 6/300, Loss: 9.5295\n",
      "Epoch 7/300, Loss: 9.5192\n",
      "Epoch 8/300, Loss: 9.5194\n",
      "Epoch 9/300, Loss: 9.5391\n",
      "Epoch 10/300, Loss: 9.5143\n",
      "Epoch 11/300, Loss: 9.5253\n",
      "Epoch 12/300, Loss: 9.5110\n",
      "Epoch 13/300, Loss: 9.5182\n",
      "Epoch 14/300, Loss: 9.5050\n",
      "Epoch 15/300, Loss: 9.5173\n",
      "Epoch 16/300, Loss: 9.5084\n",
      "Epoch 17/300, Loss: 9.4981\n",
      "Epoch 18/300, Loss: 9.5069\n",
      "Epoch 19/300, Loss: 9.5013\n",
      "Epoch 20/300, Loss: 9.5034\n",
      "Epoch 21/300, Loss: 9.5014\n",
      "Epoch 22/300, Loss: 9.4985\n",
      "Epoch 23/300, Loss: 9.5008\n",
      "Epoch 24/300, Loss: 9.5021\n",
      "Epoch 25/300, Loss: 9.4973\n",
      "Epoch 26/300, Loss: 9.5047\n",
      "Epoch 27/300, Loss: 9.5025\n",
      "Epoch 28/300, Loss: 9.5000\n",
      "Epoch 29/300, Loss: 9.5005\n",
      "Epoch 30/300, Loss: 9.4977\n",
      "Epoch 31/300, Loss: 9.4994\n",
      "Epoch 32/300, Loss: 9.5067\n",
      "Epoch 33/300, Loss: 9.4978\n",
      "Epoch 34/300, Loss: 9.5006\n",
      "Epoch 35/300, Loss: 9.5001\n",
      "Epoch 36/300, Loss: 9.4961\n",
      "Epoch 37/300, Loss: 9.4987\n",
      "Epoch 38/300, Loss: 9.4972\n",
      "Epoch 39/300, Loss: 9.4971\n",
      "Epoch 40/300, Loss: 9.4903\n",
      "Epoch 41/300, Loss: 9.5028\n",
      "Epoch 42/300, Loss: 9.4976\n",
      "Epoch 43/300, Loss: 9.4982\n",
      "Epoch 44/300, Loss: 9.4970\n",
      "Epoch 45/300, Loss: 9.4964\n",
      "Epoch 46/300, Loss: 9.4968\n",
      "Epoch 47/300, Loss: 9.4942\n",
      "Epoch 48/300, Loss: 9.4940\n",
      "Epoch 49/300, Loss: 9.4950\n",
      "Epoch 50/300, Loss: 9.4965\n",
      "Epoch 51/300, Loss: 9.4987\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 46\u001b[0m loss, _ \u001b[39m=\u001b[39m model(input_ids, labels)\n\u001b[1;32m     47\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     48\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mSimpleTransformerModel.forward\u001b[0;34m(self, src, labels)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     13\u001b[0m     src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(src) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(torch\u001b[39m.\u001b[39marange(src\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), device\u001b[39m=\u001b[39msrc\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mrepeat(src\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m))\n\u001b[0;32m---> 14\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(src)\n\u001b[1;32m     15\u001b[0m     output \u001b[39m=\u001b[39m output[:, \u001b[39m0\u001b[39m, :]  \u001b[39m# Use the first token representation for regression\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregressor(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    303\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[1;32m    305\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 306\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    308\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    309\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:573\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    571\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    572\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 573\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    574\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    576\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:581\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    580\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 581\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    582\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    583\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    584\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    585\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1189\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1176\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1177\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1187\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1190\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1191\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1192\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1193\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1194\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1195\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1196\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1197\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1198\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1199\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1201\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/functional.py:5334\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5331\u001b[0m k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m   5332\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m-> 5334\u001b[0m attn_output \u001b[39m=\u001b[39m scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n\u001b[1;32m   5335\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m tgt_len, embed_dim)\n\u001b[1;32m   5337\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Define the transformer model\n",
    "class SimpleTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, max_seq_length):\n",
    "        super(SimpleTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = nn.Embedding(max_seq_length, d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        self.regressor = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, src, labels=None):\n",
    "        src = self.embedding(src) + self.pos_encoder(torch.arange(src.size(1), device=src.device).unsqueeze(0).repeat(src.size(0), 1))\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output[:, 0, :]  # Use the first token representation for regression\n",
    "        logits = self.regressor(output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.MSELoss()(logits, labels)\n",
    "        \n",
    "        return loss, logits\n",
    "# Define the model parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = len(genus_to_idx)\n",
    "d_model = 256\n",
    "nhead = 4\n",
    "num_encoder_layers = 4\n",
    "dim_feedforward = 512\n",
    "max_seq_length = len(genus_to_idx)\n",
    "model = SimpleTransformerModel(vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, max_seq_length).to(device)\n",
    "\n",
    "# Training loop\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 300\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss, _ = model(input_ids, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f'Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'simple_transformer_model.pth')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
