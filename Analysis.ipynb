{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'otu_file' with the path to your TSV file\n",
    "otu_file = 'otu_table_psn_v35.txt'\n",
    "# wget http://downloads.hmpdacc.org/data/HMQCP/otu_table_psn_v35.txt.gz\n",
    "# Read the TSV file while skipping the first line\n",
    "otu_data = pd.read_csv(otu_file, sep='\\t', skiprows=1)\n",
    "# otu_data = pd.read_csv(\"path_to_your_output_file.csv\")\n",
    "otu_data.columns.values[0] = 'otu_id'\n",
    "\n",
    "# Define a function to parse the taxonomic data\n",
    "def parse_tax_data(otu_data, class_cols, class_regex, class_key, class_sep):\n",
    "    # Extract class information based on class_cols and class_sep\n",
    "    tax_data = otu_data[class_cols].str.split(class_sep, expand=True)\n",
    "    \n",
    "    # Create a DataFrame for taxonomic classification\n",
    "    tax_data.columns = [f'{class_key[\"hmp_rank\"]}_{i}' for i in range(tax_data.shape[1])]\n",
    "    tax_data[class_key[\"hmp_tax\"]] = tax_data.apply(lambda row: class_sep.join(row.dropna()), axis=1)\n",
    "    \n",
    "    # Merge taxonomic data back with the original OTU data\n",
    "    otu_data = otu_data.drop(columns=[class_cols])\n",
    "    otu_data = pd.concat([otu_data, tax_data], axis=1)\n",
    "    \n",
    "    return otu_data\n",
    "\n",
    "# # Apply the function to parse taxonomic data\n",
    "hmp_data = parse_tax_data(otu_data, \"Consensus Lineage\", r\"([a-z]{0,1})_{0,2}(.*)$\", {\"hmp_rank\": \"taxon_rank\", \"hmp_tax\": \"taxon_name\"}, \";\")\n",
    "\n",
    "# # Remove the regex match table (not needed in this case)\n",
    "otu_data = hmp_data.drop(columns=[col for col in hmp_data.columns if col.startswith('taxon_rank_')])\n",
    "\n",
    "# # Rename the abundance matrix to something more understandable\n",
    "otu_data.columns = ['otu_count' if col == 'otu_id' else col for col in otu_data.columns]\n",
    "# Select the top 1000 rows\n",
    "# top_1000_hmp_data = hmp_data.head(5)\n",
    "\n",
    "\n",
    "# Extract the suffix of 'g__' in the 'taxon_name' column\n",
    "otu_data['genus'] = otu_data['taxon_name'].str.extract(r'g__([^;]*)')\n",
    "otu_data = otu_data[otu_data['genus'].notna() & (otu_data['genus'] != '')]\n",
    "# # Delete the 'taxon_name' column\n",
    "otu_data = otu_data.drop(columns=['taxon_name'])\n",
    "# # Group by 'genus' and accumulate the OTU counts\n",
    "if 'genus' in otu_data.columns:\n",
    "    otu_data = otu_data.groupby('genus').sum().reset_index()\n",
    "else:\n",
    "    otu_data = otu_data  # If there is no 'genus' column, use the original data\n",
    "\n",
    "# # Print the filtered DataFrame\n",
    "# print(otu_data.head(5))\n",
    "otu_data = otu_data.drop(columns=['otu_count'])\n",
    "num_rows, num_columns = otu_data.shape\n",
    "# print(f'The DataFrame has {num_rows} rows and {num_columns} columns.')\n",
    "# Rotate the DataFrame 90 degrees clockwise\n",
    "otu_data = otu_data.transpose()\n",
    "\n",
    "# Make the first row the header\n",
    "otu_data.columns = otu_data.iloc[0]\n",
    "otu_data = otu_data[1:]\n",
    "otu_data.to_csv(\"genus_rotated.csv\", index=False)\n",
    "# Define the path for the output CSV file\n",
    "# csv_file = 'path_to_your_output_file.csv'\n",
    "\n",
    "# # Convert the DataFrame to a CSV file\n",
    "# top_1000_hmp_data.to_csv(csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.OTUDataset at 0x7f09d2fb9120>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Load the OTU table file\n",
    "otu_table = pd.read_csv('genus_rotated.csv', header=0, index_col=0)\n",
    "# print(otu_table.head(2))\n",
    "# Create a dictionary to map genus names to unique indices\n",
    "genus_names = otu_table.columns.tolist()\n",
    "genus_to_idx = {genus: idx for idx, genus in enumerate(genus_names)}\n",
    "\n",
    "# print(genus_to_idx)\n",
    "# Define a dataset class\n",
    "class OTUDataset(Dataset):\n",
    "    def __init__(self, data, genus_to_idx):\n",
    "        self.data = data\n",
    "        self.genus_to_idx = genus_to_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        counts = self.data.iloc[idx].tolist()\n",
    "        input_ids = [self.genus_to_idx[genus] for genus in self.genus_to_idx]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(counts, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Create a dataset and dataloader\n",
    "dataset = OTUDataset(otu_table, genus_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 47983.7206\n",
      "Epoch 2/300, Loss: 47668.8756\n",
      "Epoch 3/300, Loss: 47257.5199\n",
      "Epoch 4/300, Loss: 46860.5186\n",
      "Epoch 5/300, Loss: 46481.2610\n",
      "Epoch 6/300, Loss: 46063.7235\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Define the transformer model\n",
    "class SimpleTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, max_seq_length):\n",
    "        super(SimpleTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = nn.Embedding(max_seq_length, d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        self.regressor = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, src, labels=None):\n",
    "        src = self.embedding(src) + self.pos_encoder(torch.arange(src.size(1), device=src.device).unsqueeze(0).repeat(src.size(0), 1))\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output[:, 0, :]  # Use the first token representation for regression\n",
    "        logits = self.regressor(output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.MSELoss()(logits, labels)\n",
    "        \n",
    "        return loss, logits\n",
    "# Define the model parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = len(genus_to_idx)\n",
    "d_model = 256\n",
    "nhead = 4\n",
    "num_encoder_layers = 4\n",
    "dim_feedforward = 512\n",
    "max_seq_length = len(genus_to_idx)\n",
    "model = SimpleTransformerModel(vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, max_seq_length).to(device)\n",
    "\n",
    "# Training loop\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 300\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss, _ = model(input_ids, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f'Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'simple_transformer_model.pth')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
