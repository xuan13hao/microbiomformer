{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'otu_table_psn_v35.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m otu_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39motu_table_psn_v35.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39m# wget http://downloads.hmpdacc.org/data/HMQCP/otu_table_psn_v35.txt.gz\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Read the TSV file while skipping the first line\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m otu_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(otu_file, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m, skiprows\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# otu_data = pd.read_csv(\"path_to_your_output_file.csv\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m otu_data\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39motu_id\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'otu_table_psn_v35.txt'"
     ]
    }
   ],
   "source": [
    "# Replace 'otu_file' with the path to your TSV file\n",
    "otu_file = 'otu_table_psn_v35.txt'\n",
    "# wget http://downloads.hmpdacc.org/data/HMQCP/otu_table_psn_v35.txt.gz\n",
    "# Read the TSV file while skipping the first line\n",
    "otu_data = pd.read_csv(otu_file, sep='\\t', skiprows=1)\n",
    "# otu_data = pd.read_csv(\"path_to_your_output_file.csv\")\n",
    "otu_data.columns.values[0] = 'otu_id'\n",
    "\n",
    "# Define a function to parse the taxonomic data\n",
    "def parse_tax_data(otu_data, class_cols, class_regex, class_key, class_sep):\n",
    "    # Extract class information based on class_cols and class_sep\n",
    "    tax_data = otu_data[class_cols].str.split(class_sep, expand=True)\n",
    "    \n",
    "    # Create a DataFrame for taxonomic classification\n",
    "    tax_data.columns = [f'{class_key[\"hmp_rank\"]}_{i}' for i in range(tax_data.shape[1])]\n",
    "    tax_data[class_key[\"hmp_tax\"]] = tax_data.apply(lambda row: class_sep.join(row.dropna()), axis=1)\n",
    "    \n",
    "    # Merge taxonomic data back with the original OTU data\n",
    "    otu_data = otu_data.drop(columns=[class_cols])\n",
    "    otu_data = pd.concat([otu_data, tax_data], axis=1)\n",
    "    \n",
    "    return otu_data\n",
    "\n",
    "# # Apply the function to parse taxonomic data\n",
    "hmp_data = parse_tax_data(otu_data, \"Consensus Lineage\", r\"([a-z]{0,1})_{0,2}(.*)$\", {\"hmp_rank\": \"taxon_rank\", \"hmp_tax\": \"taxon_name\"}, \";\")\n",
    "\n",
    "# # Remove the regex match table (not needed in this case)\n",
    "otu_data = hmp_data.drop(columns=[col for col in hmp_data.columns if col.startswith('taxon_rank_')])\n",
    "\n",
    "# # Rename the abundance matrix to something more understandable\n",
    "otu_data.columns = ['otu_count' if col == 'otu_id' else col for col in otu_data.columns]\n",
    "# Select the top 1000 rows\n",
    "# top_1000_hmp_data = hmp_data.head(5)\n",
    "\n",
    "\n",
    "# Extract the suffix of 'g__' in the 'taxon_name' column\n",
    "otu_data['genus'] = otu_data['taxon_name'].str.extract(r'g__([^;]*)')\n",
    "otu_data = otu_data[otu_data['genus'].notna() & (otu_data['genus'] != '')]\n",
    "# # Delete the 'taxon_name' column\n",
    "otu_data = otu_data.drop(columns=['taxon_name'])\n",
    "# # Group by 'genus' and accumulate the OTU counts\n",
    "# if 'genus' in otu_data.columns:\n",
    "#     otu_data = otu_data.groupby('genus').sum().reset_index()\n",
    "# else:\n",
    "#     otu_data = otu_data  # If there is no 'genus' column, use the original data\n",
    "\n",
    "# # Print the filtered DataFrame\n",
    "# print(otu_data.head(5))\n",
    "# otu_data = otu_data.drop(columns=['otu_count'])\n",
    "num_rows, num_columns = otu_data.shape\n",
    "# print(f'The DataFrame has {num_rows} rows and {num_columns} columns.')\n",
    "# Rotate the DataFrame 90 degrees clockwise\n",
    "otu_data = otu_data.transpose()\n",
    "\n",
    "# Make the first row the header\n",
    "otu_data.columns = otu_data.iloc[0]\n",
    "otu_data = otu_data[1:]\n",
    "otu_data.to_csv(\"genus_rotated.csv\", index=False)\n",
    "# Define the path for the output CSV file\n",
    "# csv_file = 'path_to_your_output_file.csv'\n",
    "\n",
    "# # Convert the DataFrame to a CSV file\n",
    "# top_1000_hmp_data.to_csv(csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Load the OTU table file\n",
    "otu_table = pd.read_csv('genus_rotated.csv', header=0, index_col=0)\n",
    "# print(otu_table.head(2))\n",
    "# Create a dictionary to map genus names to unique indices\n",
    "genus_names = otu_table.columns.tolist()\n",
    "genus_to_idx = {genus: idx for idx, genus in enumerate(genus_names)}\n",
    "\n",
    "# print(genus_to_idx)\n",
    "# Define a dataset class\n",
    "class OTUDataset(Dataset):\n",
    "    def __init__(self, data, genus_to_idx):\n",
    "        self.data = data\n",
    "        self.genus_to_idx = genus_to_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        counts = self.data.iloc[idx].tolist()\n",
    "        input_ids = [self.genus_to_idx[genus] for genus in self.genus_to_idx]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(counts, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Create a dataset and dataloader\n",
    "dataset = OTUDataset(otu_table, genus_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 48132.0349\n",
      "Epoch 2/30, Loss: 48080.2772\n",
      "Epoch 3/30, Loss: 48061.2882\n",
      "Epoch 4/30, Loss: 48079.0227\n",
      "Epoch 5/30, Loss: 48928.5323\n",
      "Epoch 6/30, Loss: 47993.0595\n",
      "Epoch 7/30, Loss: 47992.2005\n",
      "Epoch 8/30, Loss: 47907.8407\n",
      "Epoch 9/30, Loss: 47924.0644\n",
      "Epoch 10/30, Loss: 47933.4342\n",
      "Epoch 11/30, Loss: 47854.3409\n",
      "Epoch 12/30, Loss: 47795.7758\n",
      "Epoch 13/30, Loss: 47775.6470\n",
      "Epoch 14/30, Loss: 47739.3278\n",
      "Epoch 15/30, Loss: 47696.7952\n",
      "Epoch 16/30, Loss: 47661.7442\n",
      "Epoch 17/30, Loss: 47637.8267\n",
      "Epoch 18/30, Loss: 47601.7088\n",
      "Epoch 19/30, Loss: 47585.7932\n",
      "Epoch 20/30, Loss: 47537.5703\n",
      "Epoch 21/30, Loss: 47506.9534\n",
      "Epoch 22/30, Loss: 47470.7871\n",
      "Epoch 23/30, Loss: 47466.6768\n",
      "Epoch 24/30, Loss: 47394.1270\n",
      "Epoch 25/30, Loss: 47365.2242\n",
      "Epoch 26/30, Loss: 47326.0375\n",
      "Epoch 27/30, Loss: 47292.8623\n",
      "Epoch 28/30, Loss: 47261.6105\n",
      "Epoch 29/30, Loss: 47224.5840\n",
      "Epoch 30/30, Loss: 47237.3476\n",
      "Training Time: 346.53 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Define the transformer model\n",
    "class SimpleTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, max_seq_length):\n",
    "        super(SimpleTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = nn.Embedding(max_seq_length, d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        self.regressor = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, src, labels=None):\n",
    "        src = self.embedding(src) + self.pos_encoder(torch.arange(src.size(1), device=src.device).unsqueeze(0).repeat(src.size(0), 1))\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output[:, 0, :]  # Use the first token representation for regression\n",
    "        logits = self.regressor(output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.MSELoss()(logits, labels)\n",
    "        \n",
    "        return loss, logits\n",
    "# Define the model parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = len(genus_to_idx)\n",
    "d_model = 256\n",
    "nhead = 4\n",
    "num_encoder_layers = 4\n",
    "dim_feedforward = 512\n",
    "max_seq_length = len(genus_to_idx)\n",
    "model = SimpleTransformerModel(vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, max_seq_length).to(device)\n",
    "\n",
    "# Training loop\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "num_epochs = 30\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss, _ = model(input_ids, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f'Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'simple_transformer_model.pth')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
