{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "otu_table = pd.read_csv('genus_rotated_f_filtered.csv', header=0, index_col=None)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "otu_table_scaled = scaler.fit_transform(otu_table)\n",
    "\n",
    "otu_table_scaled_df = pd.DataFrame(otu_table_scaled, columns=otu_table.columns)\n",
    "\n",
    "\n",
    "genus_names = otu_table_scaled_df.columns.tolist()\n",
    "genus_to_idx = {genus: idx for idx, genus in enumerate(genus_names)}\n",
    "\n",
    "\n",
    "otu_table_scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Akkermansia</th>\n",
       "      <th>Alistipes</th>\n",
       "      <th>Bacteroides</th>\n",
       "      <th>Blautia</th>\n",
       "      <th>Clostridium</th>\n",
       "      <th>Collinsella</th>\n",
       "      <th>Coprococcus</th>\n",
       "      <th>Dialister</th>\n",
       "      <th>Dorea</th>\n",
       "      <th>Eubacterium</th>\n",
       "      <th>...</th>\n",
       "      <th>Lachnospira</th>\n",
       "      <th>Odoribacter</th>\n",
       "      <th>Oscillospira</th>\n",
       "      <th>Parabacteroides</th>\n",
       "      <th>Phascolarctobacterium</th>\n",
       "      <th>Prevotella</th>\n",
       "      <th>Roseburia</th>\n",
       "      <th>Ruminococcus</th>\n",
       "      <th>Subdoligranulum</th>\n",
       "      <th>Sutterella</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Akkermansia  Alistipes  Bacteroides  Blautia  Clostridium  Collinsella   \n",
       "0              1          3           50        1            1            1  \\\n",
       "1              1          1           50        2            1            1   \n",
       "2              1          1           50        1            1            1   \n",
       "3              3         25           50        2            3            1   \n",
       "4              1          6           50        3            3            1   \n",
       "..           ...        ...          ...      ...          ...          ...   \n",
       "314            1          1           50        1            1            1   \n",
       "315            1          1           50        1            1            1   \n",
       "316            1          9           50        3            8            1   \n",
       "317            1          1           23        1            2            1   \n",
       "318            1          9           50        1            2            1   \n",
       "\n",
       "     Coprococcus  Dialister  Dorea  Eubacterium  ...  Lachnospira   \n",
       "0              1          1      1            1  ...            1  \\\n",
       "1              1          1      1            2  ...            1   \n",
       "2              1          1      1            1  ...            1   \n",
       "3              2          1      1            2  ...            1   \n",
       "4              1          1      1            1  ...            1   \n",
       "..           ...        ...    ...          ...  ...          ...   \n",
       "314            1          1      1            1  ...            1   \n",
       "315            1          1      1            1  ...            1   \n",
       "316            2          7      1            1  ...            1   \n",
       "317            3          9      1            1  ...            1   \n",
       "318            2          2      1            2  ...            3   \n",
       "\n",
       "     Odoribacter  Oscillospira  Parabacteroides  Phascolarctobacterium   \n",
       "0              1             1                3                      1  \\\n",
       "1              1             1                8                      1   \n",
       "2              1             1                2                      1   \n",
       "3              1            16                4                      1   \n",
       "4              1             6               25                      5   \n",
       "..           ...           ...              ...                    ...   \n",
       "314            1             1                1                      1   \n",
       "315            1             4                1                      1   \n",
       "316            1            15                1                     14   \n",
       "317            1             5                7                      1   \n",
       "318            2             8                3                      2   \n",
       "\n",
       "     Prevotella  Roseburia  Ruminococcus  Subdoligranulum  Sutterella  \n",
       "0             1          1             1                1           2  \n",
       "1             1          2             1                2           1  \n",
       "2             1          2             1                1           1  \n",
       "3             1         20             8                9           1  \n",
       "4             1         11             4                1           1  \n",
       "..          ...        ...           ...              ...         ...  \n",
       "314           1          2             1                1           3  \n",
       "315           1          1             1                1           1  \n",
       "316          11          2            13                9           1  \n",
       "317          50          1             3                1           1  \n",
       "318           1          1             9                5           1  \n",
       "\n",
       "[319 rows x 22 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "otu_table = pd.read_csv('genus_rotated_f_filtered.csv', header=0, index_col=None)\n",
    "\n",
    "# 将每一行转换为百分比\n",
    "otu_percentage = otu_table.div(otu_table.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# 设定分箱的数量，例如 5 个箱\n",
    "num_bins = 50\n",
    "\n",
    "otu_discrete = otu_percentage.apply(lambda x: pd.cut(x, bins=num_bins, labels=range(1, num_bins + 1)), axis=1)\n",
    "otu_discrete = otu_discrete.apply(pd.to_numeric)\n",
    "otu_table_scaled_df = pd.DataFrame(otu_discrete, columns=otu_table.columns)\n",
    "\n",
    "\n",
    "genus_names = otu_table_scaled_df.columns.tolist()\n",
    "genus_to_idx = {genus: idx for idx, genus in enumerate(genus_names)}\n",
    "\n",
    "\n",
    "otu_table_scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m num_layers \u001b[39m=\u001b[39m \u001b[39m6\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[39m# print(vocab_size)\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m model \u001b[39m=\u001b[39m TransformerEncoderModel(vocab_size, d_model, nhead, num_layers)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     50\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss() \n\u001b[1;32m     51\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "class OTUDataset(Dataset):# class token\n",
    "    def __init__(self, csv, mask_prob=0.15):\n",
    "        self.data = csv\n",
    "        self.mask_prob = mask_prob\n",
    "        self.data_tensor = torch.tensor(self.data.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data_tensor[idx]\n",
    "        masked_sample, mask = self.mask_data(sample)\n",
    "        return masked_sample, sample, mask\n",
    "\n",
    "    def mask_data(self, sample):\n",
    "        mask = torch.bernoulli(torch.full(sample.shape, self.mask_prob)).bool()\n",
    "        masked_sample = sample.clone()\n",
    "        masked_sample[mask] = 0  # Using 0 as a placeholder for masked values\n",
    "        return masked_sample, mask\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
    "        super(TransformerEncoderModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # include mask + 1\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)# y(i) = W*x + b, i belong to range(vocab_size)\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        x= x.long()\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x.permute(1, 0, 2))\n",
    "        x = x.permute(1, 0, 2)\n",
    "        if return_features:\n",
    "            return x\n",
    "        x = self.fc(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "vocab_size = len(genus_names)\n",
    "d_model = 64\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "# print(vocab_size)\n",
    "\n",
    "model = TransformerEncoderModel(vocab_size, d_model, nhead, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create a dataset and dataloader\n",
    "dataset = OTUDataset(otu_table_scaled_df)\n",
    "# print(dataset)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "# for  masked_data, original_data, mask in data_loader:\n",
    "#     masked_data, original_data, mask = masked_data.to(device), original_data.to(device), mask.to(device)\n",
    "#     outputs = model(masked_data)\n",
    "#     print(\"masked_data: \",masked_data[mask])\n",
    "#     print(\"original_data: \",original_data[mask])\n",
    "#     print(\"mask: \",mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [1,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[39mreturn\u001b[39;00m losses\n\u001b[1;32m     29\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m---> 30\u001b[0m losses \u001b[39m=\u001b[39m train(model, data_loader, criterion, optimizer, device)\n\u001b[1;32m     33\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mpretrained_transformer_encoder_model.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m masked_data, original_data, mask \u001b[39m=\u001b[39m masked_data\u001b[39m.\u001b[39mto(device), original_data\u001b[39m.\u001b[39mto(device), mask\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[39m=\u001b[39m model(masked_data)\n\u001b[1;32m     18\u001b[0m \u001b[39m# Flatten tensors for loss calculation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m masked_outputs \u001b[39m=\u001b[39m outputs[mask]\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 35\u001b[0m, in \u001b[0;36mTransformerEncoderModel.forward\u001b[0;34m(self, x, return_features)\u001b[0m\n\u001b[1;32m     33\u001b[0m x\u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mlong()\n\u001b[1;32m     34\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[0;32m---> 35\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(x\u001b[39m.\u001b[39;49mpermute(\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m))\n\u001b[1;32m     36\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[39mif\u001b[39;00m return_features:\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    303\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[1;32m    305\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 306\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    308\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    309\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:573\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    571\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    572\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 573\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    574\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    576\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:581\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    580\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 581\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    582\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    583\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    584\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    585\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1189\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1176\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1177\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1187\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1190\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1191\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1192\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1193\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1194\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1195\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1196\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1197\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1198\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1199\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1201\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/functional.py:5188\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   5187\u001b[0m     \u001b[39massert\u001b[39;00m in_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 5188\u001b[0m     q, k, v \u001b[39m=\u001b[39m _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n\u001b[1;32m   5189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5190\u001b[0m     \u001b[39massert\u001b[39;00m q_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/functional.py:4765\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4762\u001b[0m \u001b[39mif\u001b[39;00m k \u001b[39mis\u001b[39;00m v:\n\u001b[1;32m   4763\u001b[0m     \u001b[39mif\u001b[39;00m q \u001b[39mis\u001b[39;00m k:\n\u001b[1;32m   4764\u001b[0m         \u001b[39m# self-attention\u001b[39;00m\n\u001b[0;32m-> 4765\u001b[0m         proj \u001b[39m=\u001b[39m linear(q, w, b)\n\u001b[1;32m   4766\u001b[0m         \u001b[39m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[1;32m   4767\u001b[0m         proj \u001b[39m=\u001b[39m proj\u001b[39m.\u001b[39munflatten(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, (\u001b[39m3\u001b[39m, E))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "'''\n",
    "d_model = 4\n",
    "output = [\n",
    "    [2,2,4,5]   #token 0\n",
    "    [1,3,4,5]   #token 1\n",
    "    [4,1,3,6]   #token 2\n",
    "]\n",
    "'''\n",
    "def train(model, data_loader, criterion, optimizer, device, epochs=32):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for masked_data, original_data, mask in data_loader:\n",
    "            masked_data, original_data, mask = masked_data.to(device), original_data.to(device), mask.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(masked_data)\n",
    "            # Flatten tensors for loss calculation\n",
    "            masked_outputs = outputs[mask]\n",
    "            masked_targets = original_data[mask].long()\n",
    "            loss = criterion(masked_outputs, masked_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss}')\n",
    "    return losses\n",
    "torch.cuda.empty_cache()\n",
    "losses = train(model, data_loader, criterion, optimizer, device)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'pretrained_transformer_encoder_model.pth')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(losses) + 1), losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m masked_sample, original_sample, mask \u001b[39m=\u001b[39m masked_sample\u001b[39m.\u001b[39mto(device), original_sample\u001b[39m.\u001b[39mto(device), mask\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     94\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 95\u001b[0m output \u001b[39m=\u001b[39m model(masked_sample)\n\u001b[1;32m     96\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, vocab_size)\n\u001b[1;32m     97\u001b[0m original_sample \u001b[39m=\u001b[39m original_sample\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 62\u001b[0m, in \u001b[0;36mTransformerEncoderModel.forward\u001b[0;34m(self, x, return_features)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, return_features\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     61\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mlong()\n\u001b[0;32m---> 62\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(x)\n\u001b[1;32m     63\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_encoder(x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[1;32m     64\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device\n",
    "device = torch.device( \"cpu\")\n",
    "\n",
    "# Load and preprocess the data\n",
    "otu_table = pd.read_csv('genus_rotated_f_filtered.csv', header=0, index_col=None)\n",
    "\n",
    "# Convert each row to percentage\n",
    "otu_percentage = otu_table.div(otu_table.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Set the number of bins\n",
    "num_bins = 50\n",
    "\n",
    "# Discretize the data\n",
    "otu_discrete = otu_percentage.apply(lambda x: pd.cut(x, bins=num_bins, labels=range(1, num_bins + 1)), axis=1)\n",
    "otu_discrete = otu_discrete.apply(pd.to_numeric)\n",
    "otu_table_scaled_df = pd.DataFrame(otu_discrete, columns=otu_table.columns)\n",
    "\n",
    "# Generate genus to index mapping\n",
    "genus_names = otu_table_scaled_df.columns.tolist()\n",
    "genus_to_idx = {genus: idx for idx, genus in enumerate(genus_names)}\n",
    "\n",
    "# Define the dataset class\n",
    "class OTUDataset(Dataset):\n",
    "    def __init__(self, csv, mask_prob=0.15):\n",
    "        self.data = csv\n",
    "        self.mask_prob = mask_prob\n",
    "        self.data_tensor = torch.tensor(self.data.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data_tensor[idx]\n",
    "        masked_sample, mask = self.mask_data(sample)\n",
    "        return masked_sample, sample, mask\n",
    "\n",
    "    def mask_data(self, sample):\n",
    "        mask = torch.bernoulli(torch.full(sample.shape, self.mask_prob)).bool()\n",
    "        masked_sample = sample.clone()\n",
    "        masked_sample[mask] = 0  # Using 0 as a placeholder for masked values\n",
    "        return masked_sample, mask\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
    "        super(TransformerEncoderModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, d_model)  # Include mask + 1\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)  # y(i) = W*x + b, i belong to range(vocab_size)\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        x = x.long()\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x.permute(1, 0, 2))\n",
    "        x = x.permute(1, 0, 2)\n",
    "        if return_features:\n",
    "            return x\n",
    "        x = self.fc(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n",
    "# Set parameters for the model\n",
    "vocab_size = len(genus_names)\n",
    "d_model = 64\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "\n",
    "# Instantiate the model, criterion, and optimizer\n",
    "model = TransformerEncoderModel(vocab_size, d_model, nhead, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create a dataset and dataloader\n",
    "dataset = OTUDataset(otu_table_scaled_df)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for masked_sample, original_sample, mask in data_loader:\n",
    "        masked_sample, original_sample, mask = masked_sample.to(device), original_sample.to(device), mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(masked_sample)\n",
    "        output = output.view(-1, vocab_size)\n",
    "        original_sample = original_sample.view(-1)\n",
    "        mask = mask.view(-1)\n",
    "        loss = criterion(output[mask], original_sample[mask].long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class EnhancedMLP(nn.Module):\n",
    "    def __init__(self, pretrained_model, input_size, hidden_size, num_classes):\n",
    "        super(EnhancedMLP, self).__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     with torch.no_grad():\n",
    "    #         x = self.pretrained_model(x.long(), return_features=True)  # 获取编码后的特征\n",
    "    #     x = x.mean(dim=1)  # 对序列维度进行平均池化\n",
    "    #     x = torch.nn.functional.relu(self.fc1(x))\n",
    "    #     x = self.fc2(x)\n",
    "    #     return x\n",
    "    def forward(self, x):\n",
    "        x = self.pretrained_model(x, return_features=True)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerEncoderModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 加载预训练的TransformerEncoderModel\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pretrained_model \u001b[39m=\u001b[39m TransformerEncoderModel(\u001b[39m22\u001b[39m, d_model, nhead, num_layers)\n\u001b[1;32m      3\u001b[0m pretrained_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mpretrained_transformer_encoder_model.pth\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m pretrained_model\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TransformerEncoderModel' is not defined"
     ]
    }
   ],
   "source": [
    "# 加载预训练的TransformerEncoderModel\n",
    "pretrained_model = TransformerEncoderModel(22, d_model, nhead, num_layers)\n",
    "pretrained_model.load_state_dict(torch.load('pretrained_transformer_encoder_model.pth'))\n",
    "pretrained_model.to(device)\n",
    "pretrained_model.eval()  # 设置为评估模式\n",
    "\n",
    "# EnhancedMLP模型参数\n",
    "input_size = vocab_size * d_model  # 根据TransformerEncoderModel的输出尺寸调整\n",
    "hidden_size = 128\n",
    "num_classes = 10  # 假设有10个类别进行分类\n",
    "\n",
    "# 初始化EnhancedMLP模型并移动到GPU\n",
    "enhanced_mlp = EnhancedMLP(pretrained_model, input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(enhanced_mlp.parameters(), lr=0.0001)\n",
    "enhanced_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "df = pd.read_csv(\"NSCLC.csv\")\n",
    "# Filter out columns that contain all zeros\n",
    "# df = df.loc[:, (NSCLC != 0).any(axis=0)]\n",
    "f1 = pd.read_csv('genus_rotated_f_filtered.csv')\n",
    "# print(f1.shape)\n",
    "# Extract genus-level data\n",
    "df['Genus'] = df['#NAME'].apply(lambda x: x.split(';g__')[1].split(';')[0] if ';g__' in x else 'Unclassified')\n",
    "\n",
    "# Select the relevant columns\n",
    "genus_df = df[['Genus'] + df.columns[1:-1].tolist()]\n",
    "\n",
    "# Filter out rows with \"_unclassified\" in the Genus column\n",
    "NSCLC = genus_df[~genus_df['Genus'].str.contains('_unclassified')]\n",
    "if 'Genus' in NSCLC.columns:\n",
    "    NSCLC = NSCLC.groupby('Genus').sum().reset_index()\n",
    "else:\n",
    "    NSCLC = NSCLC  # If there is no 'genus' column, use the original data\n",
    "NSCLC = NSCLC[NSCLC['Genus'].notna() & (NSCLC['Genus'] != '')]\n",
    "NSCLC = NSCLC.loc[:, (NSCLC != 0).any(axis=0)]\n",
    "NSCLC.set_index(NSCLC.columns[0], inplace=True)\n",
    "f2 = NSCLC.transpose()\n",
    "missing_cols = [col for col in f1.columns if col not in f2.columns]\n",
    "# Add missing columns to f2 with values set to 0 using pd.concat\n",
    "f2 = pd.concat([f2, pd.DataFrame(0, index=f2.index, columns=missing_cols)], axis=1)\n",
    "# Drop columns from f2 that are not in f1\n",
    "f2 = f2[f1.columns]\n",
    "# Merge f2 to f1, keeping only the column names\n",
    "f1 = f2\n",
    "metadata  = pd.read_csv('metadata_response.csv')\n",
    "metadata.set_index(metadata.columns[0], inplace=True)\n",
    "# num_columns = len(merged_table.columns) - 1\n",
    "merged_table = f1.join(metadata, how='inner')\n",
    "# merged_table.to_csv(\"merged_table.csv\",index=False)\n",
    "# merged_table = merged_table.drop(columns=['Best response'])\n",
    "response = merged_table['Best response']\n",
    "otu_table_merge = merged_table.drop(columns=['Best response'])\n",
    "# Drop the first column if it contains sample IDs or unnecessary data\n",
    "otu_table_merge = otu_table_merge.iloc[:, 1:]\n",
    "\n",
    "# # Normalize OTU counts by total counts per sample\n",
    "# normalized_otu_counts = otu_table_merge.div(otu_table_merge.sum(axis=1), axis=0)\n",
    "\n",
    "# # Optionally, convert to percentages\n",
    "# normalized_otu_counts *= 100\n",
    "# 标准化OTU数据\n",
    "scaler = StandardScaler()\n",
    "otu_table_scaled = scaler.fit_transform(otu_table_merge)\n",
    "# normalized_otu_counts = otu_table_merge\n",
    "# 将标准化后的数据转换回DataFrame\n",
    "normalized_otu_counts = pd.DataFrame(otu_table_scaled, columns=otu_table_merge.columns)\n",
    "# Print to verify\n",
    "# normalized_otu_counts.to_csv(\"normalized_otu_counts.csv\",index=False)\n",
    "# Create a dictionary to map genus names to unique indices\n",
    "genus_names = normalized_otu_counts.columns.tolist()\n",
    "genus_to_idx = {genus: idx for idx, genus in enumerate(genus_names)}\n",
    "genus_names\n",
    "genus_to_idx\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'merged_table' is your DataFrame containing the response and features\n",
    "encoder = LabelEncoder()\n",
    "merged_table['Best response'] = encoder.fit_transform(merged_table['Best response'])\n",
    "\n",
    "# Separate features and target\n",
    "# features = merged_table.drop('Best response', axis=1)\n",
    "features = normalized_otu_counts\n",
    "targets = merged_table['Best response']\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OTUDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure data is returned as tensors\n",
    "        x = torch.tensor(self.features.iloc[idx].values, dtype=torch.float32)\n",
    "        y = torch.tensor(self.targets.iloc[idx], dtype=torch.long)  # Use torch.long for classification labels\n",
    "        return x, y\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Encode the 'Best response' column\n",
    "encoder = LabelEncoder()\n",
    "merged_table['Best response'] = encoder.fit_transform(merged_table['Best response'])\n",
    "\n",
    "# Split the features and targets into training and testing sets\n",
    "features_train, features_test, targets_train, targets_test = train_test_split(\n",
    "    features, targets, test_size=0.2, random_state=42)\n",
    "# print(features_test)\n",
    "train_dataset = OTUDataset(features_train, targets_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataset = OTUDataset(features_test, targets_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the enhanced model\n",
    "def train_and_evaluate(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs=256):\n",
    "    # Check if GPU is available and move the model to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    def train_model(model, criterion, optimizer, dataloader, num_epochs):\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for inputs, labels in dataloader:\n",
    "                # Move inputs and labels to GPU\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader):.4f}')\n",
    "\n",
    "    def evaluate_model(model, dataloader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                # Move inputs and labels to GPU\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, criterion, optimizer, train_dataloader,device,num_epochs)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    evaluate_model(model, test_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "enhanced_model = EnhancedMLP(pretrained_model, d_model, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(enhanced_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate the enhanced model\n",
    "train_and_evaluate(enhanced_model, train_dataloader, test_dataloader, criterion, optimizer,num_epochs=64)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
